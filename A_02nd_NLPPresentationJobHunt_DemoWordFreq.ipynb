{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d10fc3",
   "metadata": {},
   "source": [
    "# Job-Hunt NLP Demo\n",
    "\n",
    "Which demo will also be useful in doing some quick NLP work to see how my résumé's word distribution matches that from job descriptions.\n",
    "\n",
    "There's a wonderful project out there, [MyBinder](https://mybinder.org), which allows you to interactively run a Jupyter notebook completely online. It's nice to have when you'd like to play with code and see better the outputs that come from running that code. I've had some problems with images going down, but I'm going to work to keep this one up.\n",
    "\n",
    "The link to the online, interactive notebook - the binder - is at the badge you see right here\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/bballdave025/job-app-word-freq/main?labpath=A_02nd_NLPPresentationJobHunt_DemoWordFreq.ipynb)\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## We are calling this version 0.1.002\n",
    "\n",
    "It's for the FamilySearch CJKV jobs applied for in August 2023.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f69f9",
   "metadata": {},
   "source": [
    "## Link to setup from the Conda Prompt\n",
    "\n",
    "The instructions for setting up the conda environment from Windows is in [my first version, here on GitHub](https://github.com/bballdave025/job-app-word-freq/blob/main/A_v01_NLP_Presentation_Job_Hunt_NLP_Useful_Demo_Word_Freq.ipynb). Soon, I will figure out how to make the [MyBinder](https://mybinder.org) server <strike>[MyBinder server]()</strike> for that first version persistent, and you can look at all the setup stuff there. [![Binder](./badge_logo_dwb_v_0-1-001_merged_small.png)](https://mybinder.org/v2/gh/bballdave025/job-app-word-freq/original-timed-freq?labpath=A_v01_NLP_Presentation_Job_Hunt_NLP_Useful_Demo_Word_Freq.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d3fc8",
   "metadata": {},
   "source": [
    "## The Second Iteration\n",
    "\n",
    "### Several FamilySearch Résumés\n",
    "\n",
    "My friend at the [FamilySearch Library](https://www.familysearch.org/en/library/) let me know about a few job availabilities. These are all with a group - of which he and I are part - of missionaries and volunteers who have been working on [CJKV (Chinese, Japanese, Korean, Vietnamese)-character](https://en.wikipedia.org/wiki/CJKV_characters) handwriting and block-print recognition. I already put in the applications with résumés, but all résumés are pretty simple. I'm going to see how the different job descriptions compare to the résumés as regards the word-frequency distribution. \n",
    "\n",
    "I'm going to add some improvements to my first, time-limited version. These include the better-presentation output of the word and frequency arrays. I would also like to add something that removes small words that serve a more grammatical function; in the [NLTK book](https://www.nltk.org/book/) (Officially: BIRD, Steven; KLEIN, Ewan; and LOPER, Edward, Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit, retrieved 2023-08-07, https://web.archive.org/web/20230721043038/https://www.nltk.org/book/)\n",
    "Steven Bird, Ewan Klein, and Edward Loper), these are called stopwords (cf. Chapter 2). Perhaps I'll put in some n-gram comparison, but what I'd really like to do are space-separated bigrams - now I've looked it up, and the official term is Orthogonal Sparse Bigrams (OSBs), cf [this paper](https://web.archive.org/web/20230807215040/https://www.siefkes.net/papers/winnow-spam.pdf). The paper is also in this directory as `siefkes_winnow_osb.pdf`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2201e56",
   "metadata": {},
   "source": [
    "## Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d411d8",
   "metadata": {},
   "source": [
    "The text of my résumé for these jobs is in the local file,\n",
    "\n",
    "```\n",
    "res_CJKV.txt\n",
    "```\n",
    "\n",
    "the job descriptions for the jobs are in local files as well, specifically,\n",
    "\n",
    "```\n",
    "desc_CJKV_dev3.txt\n",
    "desc_CJKV_dev4.txt\n",
    "desc_CJKV_dev5.txt\n",
    "desc_CJKV_devInTest3.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b18797",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_text_filenames = \\\n",
    "  [\"res_CJKV.txt\",\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb355d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_text_filenames = \\\n",
    "  [\"desc_CJKV_dev5.txt\",\n",
    "   \"desc_CJKV_dev4.txt\",\n",
    "   \"desc_CJKV_dev3.txt\",\n",
    "   \"desc_CJKV_devInTest3.txt\",\n",
    "  ]\n",
    "\n",
    "# The \"dev5\" is the nicest job - and it's with Java, which I know best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca5e0c",
   "metadata": {},
   "source": [
    "`######################################################`\n",
    "\n",
    "The job description page looks to contain something like `JavaScript`, `ajax`, etc.\n",
    "\n",
    "Rather than writing in a webscraper or looking through the code and finding what gets pulled from the database, I'm just going to copy/paste the text into the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Code to get current timestamp, if needed.\n",
    "##+ Meant to be run once, then commented out.\n",
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c57519",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_job_desc_filenames = job_description_text_filenames\n",
    "local_job_appl_filenames = application_text_filenames\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(local_job_desc_filenames)\n",
    "print()\n",
    "pprint.pprint(local_job_appl_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ceffac",
   "metadata": {},
   "source": [
    "Output was\n",
    "\n",
    "```\n",
    "['desc_CJKV_dev5.txt',\n",
    " 'desc_CJKV_dev4.txt',\n",
    " 'desc_CJKV_dev3.txt',\n",
    " 'desc_CJKV_devInTest3.txt']\n",
    "\n",
    "['res_CJKV.txt']\n",
    "```\n",
    "\n",
    "at `1691423942_20230807T155902-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in texts to original strings\n",
    "\n",
    "def read_in_texts(local_desc_fnames, local_appl_fnames,\n",
    "                  do_combine_desc_files = False,\n",
    "                  do_combine_appl_files = True\n",
    "                 ):\n",
    "    for_pairwise_desc_texts = []\n",
    "    for_pairwise_appl_texts = []\n",
    "    \n",
    "    complete_description_text = \"\"\n",
    "    complete_application_text = \"\"\n",
    "    \n",
    "    if do_combine_desc_files:\n",
    "        complete_description_text = \" \"\n",
    "    ##endof:  if do_combine-desc_files\n",
    "    \n",
    "    if do_combine_appl_files:\n",
    "        complete_application_text = \" \"\n",
    "    ##endof:  if do_combine_appl_files\n",
    "    \n",
    "    for this_description_filename in local_job_desc_filenames:\n",
    "        with open(this_description_filename, 'r', encoding='utf-8') as dfh:\n",
    "            this_desc_file_content_str = dfh.read()\n",
    "            if do_combine_desc_files:\n",
    "                complete_description_text += \" \" + this_desc_file_content_str\n",
    "            else:\n",
    "                this_desc_in_array_str = \" \" + this_desc_file_content_str + \" \"\n",
    "                for_pairwise_desc_texts.append(this_desc_in_array_str)\n",
    "            ##endof:  if/else do_combine_desc_files\n",
    "        ##endof:  with open ... dfh\n",
    "    ##endof:  for this_description_filename in local_job_desc_filenames\n",
    "    \n",
    "    for this_application_filename in local_job_appl_filenames:\n",
    "        with open(this_application_filename, 'r', encoding='utf-8') as afh:\n",
    "            this_appl_file_content_str = afh.read()\n",
    "            if do_combine_appl_files:\n",
    "                complete_application_text += \" \" + this_appl_file_content_str\n",
    "            else:\n",
    "                this_appl_in_array_str = \" \" + this_appl_file_content_str + \" \"\n",
    "                for_pairwise_appl_texts.append(this_appl_in_array_str)\n",
    "            ##endof:  if/else do_combine_appl_files\n",
    "        ##endof:  with open ... afh\n",
    "    ##endof:  for this_application_filename in local_job_appl_filenames\n",
    "    \n",
    "    complete_description_text += \" \"\n",
    "    complete_application_text += \" \"\n",
    "    \n",
    "    if do_combine_desc_files:\n",
    "        for_pairwise_desc_texts = [complete_description_text]\n",
    "    ##endof:  if do_combine-desc_files\n",
    "    \n",
    "    if do_combine_appl_files:\n",
    "        for_pairwise_appl_texts = [complete_application_text]\n",
    "    ##endof:  if do_combine_appl_files\n",
    "    \n",
    "    return for_pairwise_desc_texts, for_pairwise_appl_texts\n",
    "    \n",
    "##endof:  read_in_texts(<params>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8cb215",
   "metadata": {},
   "source": [
    "#### This next, make_it_one_line_single_spaced function will be very useful as we go forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df24df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "def make_it_one_line_single_spaced(input_str):\n",
    "    processing_str = input_str\n",
    "    \n",
    "    processing_str = ' '.join(processing_str.split())\n",
    "    processing_str = processing_str.replace(\"\\t\", \" \")\n",
    "    processing_str = processing_str.replace(\"\\n\", \" \")\n",
    "    processing_str = re.sub(r\"(^|[^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    return processing_str\n",
    "##endof:  make_it_one_line_single_spaced(input_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0b2ce5",
   "metadata": {},
   "source": [
    "### The actual reading in of the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ca193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_description_text, complete_application_text = \\\n",
    "                    read_in_texts(local_job_desc_filenames,\n",
    "                                  local_job_appl_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e6c18",
   "metadata": {},
   "source": [
    "### Code for cleaning text\n",
    "\n",
    "We will iterate a bit, so as not to have to write a text normalizer for the whole world. Rather than putting together regexes to test for things like which contractions are there and which other things might need changing (especially things like dashes), I'm doing simple regexes. Q&R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c83bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#from bs4 import UnicodeDammit\n",
    "\n",
    "def clean_text_string_quickly(input_str):\n",
    "    processing_str = input_str\n",
    "    \n",
    "    # ## one line, single-spaced\n",
    "    # processing_str = ' '.join(processing_str.split())\n",
    "    # processing_str = processing_str.replace(\"\\t\", \" \")\n",
    "    # processing_str = processing_str.replace(\"\\n\", \" \")\n",
    "    # processing_str = re.sub(r\"(^|[^ ])[ ][ ]+($|[^ ])\",\n",
    "    #                         r\"\\g<1> \\g<2>\",\n",
    "    #                         processing_str,\n",
    "    #                         flags=re.IGNORECASE\n",
    "    #                       )\n",
    "    \n",
    "    ## one line, single-spaced\n",
    "    processing_str = make_it_one_line_single_spaced(processing_str)\n",
    "    \n",
    "    \n",
    "    ## get rid of outside-ascii (or control character)\n",
    "    processing_str = re.sub(r\"[^\\u0020-\\u007E]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## my stuff\n",
    "    processing_str = re.sub(r\"[ ][|]+[ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    processing_str = processing_str.replace(r\"&\", \"and\")\n",
    "    processing_str = processing_str.replace(r\"U.S.\", \"U S \")\n",
    "    \n",
    "    ## get rid of punctuation\n",
    "    processing_str = re.sub(r\"(([^0-9 ])[.,!?:\\\"']([) ]|$))\",\n",
    "                            r\"\\g<2>\\g<3>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    processing_str = re.sub(r\"(([0-9 ])[.,!?:\\\"']([ ]|$))\",\n",
    "                            r\"\\g<2>\\g<3>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    # parentheses\n",
    "    processing_str = processing_str.replace(r\"(\", \" \")\n",
    "    processing_str = processing_str.replace(r\")\", \" \")\n",
    "    # dashes\n",
    "    processing_str = re.sub(r\"[ ][-]+[ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ##  lowercase - to skip until a few iterations through\n",
    "    ##+ cleaning the text\n",
    "    processing_str = processing_str.casefold()\n",
    "    \n",
    "    ## fixes found by iterating this cleaning function\n",
    "    processing_str = re.sub(r\"[ ][/][ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## What's found in the documents\n",
    "    # My inspection\n",
    "    processing_str = processing_str.replace(r\"s.r\", \"s r\")\n",
    "    processing_str = processing_str.replace(r\"c++/perl\", \"c++ perl\")\n",
    "    \n",
    "    # From the automated looking, below\n",
    "    processing_str = processing_str.replace(\n",
    "                                 r\"monitors/equipment\", \n",
    "                                  \"monitors equipment\"\n",
    "    )\n",
    "    processing_str = processing_str.replace(\n",
    "                                 r\"product/engineering\", \n",
    "                                  \"product engineering\"\n",
    "    )\n",
    "    processing_str = processing_str.replace(\n",
    "                                 r\"engineering/troubleshooting\", \n",
    "                                  \"engineering troubleshooting\"\n",
    "    )\n",
    "    processing_str = processing_str.replace(\n",
    "                                 r\"engineering/programming\", \n",
    "                                  \"engineering programming\"\n",
    "    )\n",
    "    processing_str = processing_str.replace(\n",
    "                         r\"analytical/diagnostic/troubleshooting\", \n",
    "                          \"analytical diagnostic troubleshooting\"\n",
    "    )\n",
    "    processing_str = processing_str.replace(\n",
    "                                 r\"integration/continuous\", \n",
    "                                  \"integration continuous\"\n",
    "    )\n",
    "    \n",
    "    processing_str = processing_str.replace(r\"net/powershell\", \n",
    "                                                \"net powershell\")\n",
    "    processing_str = processing_str.replace(r\"c/c\", \"c c\")\n",
    "    \n",
    "    # KEEP THESE 3 EXAMPLES IN THE CODE FOR COPY/PASTE, WHATEVER\n",
    "    # processing_str = processing_str.replace(r\"notice/more\", \n",
    "    #                                               \"notice more\")\n",
    "    # processing_str = processing_str.replace(r\"s.r\", \"s r\")\n",
    "    # processing_str = processing_str.replace(\n",
    "    #                              r\"monitors/equipment\", \n",
    "    #                               \"monitors equipment\"\n",
    "    # )\n",
    "    \n",
    "    # ##spacing fix at the end\n",
    "    # processing_str = re.sub(r\"(^|[^ ])[ ][ ]+($|[^ ])\",\n",
    "    #                         r\"\\g<1> \\g<2>\",\n",
    "    #                         processing_str,\n",
    "    #                         flags=re.IGNORECASE\n",
    "    #                        )\n",
    "    \n",
    "    ## spacing fix at the end\n",
    "    processing_str = make_it_one_line_single_spaced(processing_str)\n",
    "    \n",
    "    ## Let's give it back\n",
    "    return processing_str\n",
    "\n",
    "##endof:  clean_text_string_quickly(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6bb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_stopwords(input_str):\n",
    "    ##  From https://www.nltk.org/book/ch02.html\n",
    "    ##+ > [Stopwords are] high-frequency words like the, to and also that we \n",
    "    ##+ > sometimes want to filter out of a document before further processing. \n",
    "    ##+ > Stopwords usually have little lexical content, and their presence in \n",
    "    ##+ > a text fails to distinguish it from other texts.\n",
    "    \n",
    "    processing_str = input_str\n",
    "    \n",
    "    stopwords_to_remove = [\n",
    "'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 'can', 'will', 'just', 'should', 'now'\n",
    "]\n",
    "    \n",
    "    # No attempt to optimize code, here. Q&R\n",
    "    for my_stopword in stopwords_to_remove:\n",
    "        #processing_str = processing_str.replace(my_stopword, \" \")\n",
    "        word_with_boundaries = r\"\\b\" + my_stopword + r\"\\b\"\n",
    "        processing_str = re.sub(word_with_boundaries, \" \", \n",
    "                                processing_str, \n",
    "                                flags=re.IGNORECASE)\n",
    "    ##endof:  for my_stopword in stopwords_to_remove\n",
    "    \n",
    "    return processing_str\n",
    "##endof:  remove_stopwords(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll usually keep these two.\n",
    "do_look_at_description_text = True\n",
    "do_look_at_application_text = True\n",
    "\n",
    "#  this one can go (False) if you don't want the big strings\n",
    "#+ i.e. you don't want the complete file contents\n",
    "do_print_the_big_strings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_description_text:\n",
    "    test1 = []\n",
    "    for desc_text_str in complete_description_text:\n",
    "        test1.append(clean_text_string_quickly(desc_text_str))\n",
    "        if do_print_the_big_strings:\n",
    "            import pprint\n",
    "            pprint.pprint(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    test2 = []\n",
    "    for appl_text_str in complete_application_text:\n",
    "        test2.append(clean_text_string_quickly(appl_text_str))\n",
    "        if do_print_the_big_strings:\n",
    "            import pprint\n",
    "            pprint.pprint(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b746d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Without stopwords\n",
    "test1 = [make_it_one_line_single_spaced(\n",
    "                                remove_stopwords(their_text)\n",
    "                                       ) for their_text in test1\n",
    "        ]\n",
    "\n",
    "if do_print_the_big_strings:\n",
    "    import pprint\n",
    "    pprint.pprint(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400fcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = [make_it_one_line_single_spaced(\n",
    "                                remove_stopwords(their_text)\n",
    "                                       ) for their_text in test2\n",
    "        ]\n",
    "\n",
    "if do_print_the_big_strings:\n",
    "    import pprint\n",
    "    pprint.pprint(test2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03a2ca6",
   "metadata": {},
   "source": [
    "### For these next few cells, we are finding things to search and replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at contractions\n",
    "if do_look_at_description_text:\n",
    "    for their_text in test1:\n",
    "        print()\n",
    "        print(re.findall(r\"\\b('[\\w']+\\b|[\\w']+'[\\w']+|[\\w']+')\\b\",\n",
    "                         their_text)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afe99f",
   "metadata": {},
   "source": [
    "First run-through had\n",
    "\n",
    "```\n",
    "[\"organization's\", \"bachelor's\", \"master's\"]\n",
    "\n",
    "[\"bachelor's\"]\n",
    "\n",
    "[\"bachelor's\"]\n",
    "\n",
    "[\"bachelor's\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    for my_text in test2:\n",
    "        print()\n",
    "        print(re.findall(r\"\\b('[\\w']+\\b|[\\w']+'[\\w']+|[\\w']+')\\b\", \n",
    "                         my_text)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0c8af",
   "metadata": {},
   "source": [
    "First run-through had\n",
    "\n",
    "```\n",
    "[\"workplace's\", \"wife's\", \"nist's\", \"container's\", \"mission's\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at all slashes\n",
    "if do_look_at_description_text:\n",
    "    for their_text in test1:\n",
    "        print()\n",
    "        print(re.findall(r\"\\b[\\w/]+/[\\w/]+\\b\", \n",
    "                         their_text)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb06226",
   "metadata": {},
   "source": [
    "First run-through had\n",
    "\n",
    "```\n",
    "['monitors/equipment']\n",
    "\n",
    "['product/engineering', 'engineering/troubleshooting', 'monitors/equipment']\n",
    "\n",
    "['monitors/equipment']\n",
    "\n",
    "['engineering/programming', 'analytical/diagnostic/troubleshooting', 'monitors/equipment', 'integration/continuous']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eaeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    for my_text in test2:\n",
    "        print()\n",
    "        print(re.findall(r\"\\b[\\w./]+/[\\w/]+\\b\",\n",
    "                         my_text)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872dd7f6",
   "metadata": {},
   "source": [
    "First run-through had\n",
    "\n",
    "```\n",
    "['github.com/bballdave025', 'stackexchange.com/users/8693193', 'net/powershell', 'c/c']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593ac2f",
   "metadata": {},
   "source": [
    "## Word Frequency Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f8fb1",
   "metadata": {},
   "source": [
    "I want to use an `OrderedDict`, rather than mess with sorting the contents of a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_sorted_word_counts(*cleaned_strings):\n",
    "                           #,\n",
    "                           #do_output_sorted_file=False,\n",
    "                           #sorted_filename=\\\n",
    "                           #    \"sorted_words_from_strings.txt\"):\n",
    "    '''\n",
    "    @return  OrderedDict\n",
    "    '''\n",
    "    \n",
    "    EXIT_NOWORDSWEREFOUND = -1\n",
    "    \n",
    "    work_with_str = combine_strings(cleaned_strings)\n",
    "    \n",
    "    list_of_words_in_str = work_with_str.split()\n",
    "    \n",
    "    if len(list_of_words_in_str) <= 0:\n",
    "        print(\"No words were found.\", file=sys.stderr)\n",
    "        print(\"The program will exit.\", file=sys.stderr)\n",
    "        #sys.exit(EXIT_NOWORDSWEREFOUND)\n",
    "        return EXIT_NOWORDSWEREFOUND\n",
    "    ##endof:  if len(list_of_words_in_str) <= 0\n",
    "    \n",
    "    word_count_ordered_dict = OrderedDict()\n",
    "    \n",
    "    for this_word in list_of_words_in_str:\n",
    "        if this_word in word_count_ordered_dict:\n",
    "            word_count_ordered_dict[this_word] += 1\n",
    "        else:\n",
    "            word_count_ordered_dict[this_word] = 1\n",
    "        ##endof:  if/else this_word in list_of_words_in_str\n",
    "    ##endof:  for this_word in list_of_words_in_str\n",
    "    \n",
    "    ## DWB note ##\n",
    "    ##  At this point, the OrderedDict is sorted by the\n",
    "    ##+ order in which keys were inserted, not by their\n",
    "    ##+ count.\n",
    "    \n",
    "    for key, _ in \\\n",
    "          sorted(word_count_ordered_dict.items(),\n",
    "                 key=lambda word_and_count: word_and_count[1],\n",
    "                 reverse=True):\n",
    "        word_count_ordered_dict.move_to_end(key)\n",
    "    ##endof:  for myword, _ ...\n",
    "    \n",
    "    return word_count_ordered_dict\n",
    "    \n",
    "##endof:  get_sorted_word_counts(*cleaned_strings)\n",
    "\n",
    "def combine_strings(tuple_of_strings):\n",
    "                    #, \n",
    "                    #do_output_raw_file=False,\n",
    "                    #raw_filename='raw_words_from_strings.txt'):\n",
    "    '''\n",
    "    @return  string\n",
    "    '''\n",
    "    \n",
    "    returned_str = \" \"\n",
    "    \n",
    "    for this_str in tuple_of_strings:\n",
    "        returned_str += this_str + \" \"\n",
    "    ##endof:  for this_str in tuple_of_string\n",
    "    \n",
    "    ## one line, single-spaced\n",
    "    returned_str = ' '.join(returned_str.split())\n",
    "    returned_str = returned_str.replace(\"\\t\", \" \")\n",
    "    returned_str = returned_str.replace(\"\\n\", \" \")\n",
    "    returned_str = re.sub(r\"([^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            returned_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    return returned_str\n",
    "##endof:  combine_strings(tuple_of_strings)\n",
    "\n",
    "# @TODO: add a sort-by-word as well as sort-by-count flag\n",
    "# @TODO:  also, print out the pre-sorted and sorted files\n",
    "#       + with word lists, frequency, and in-order-of-\n",
    "#       + highest-count stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9c0c0b",
   "metadata": {},
   "source": [
    "**Only do the two below if you want a big preview! What I'm saying is, \"The two cells below will give you long outputs if uncommented.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242cc32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_description_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa130a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_application_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa30af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning and Counting ##\n",
    "\n",
    "description_strings_pre = \\\n",
    "  [clean_text_string_quickly(this_desc_text_str) \n",
    "           for this_desc_text_str in complete_description_text]\n",
    "application_strings_pre = \\\n",
    "  [clean_text_string_quickly(this_appl_text_str)\n",
    "           for this_appl_text_str in complete_application_text]\n",
    "\n",
    "description_strings = \\\n",
    "  [make_it_one_line_single_spaced(remove_stopwords(this_desc)) \n",
    "           for this_desc in description_strings_pre]\n",
    "application_strings = \\\n",
    "  [make_it_one_line_single_spaced(remove_stopwords(this_appl)) \n",
    "           for this_appl in application_strings_pre]\n",
    "\n",
    "description_word_counts = \\\n",
    "  [get_sorted_word_counts(description_str) \\\n",
    "              for description_str in description_strings]\n",
    "application_word_counts = \\\n",
    "  [get_sorted_word_counts(application_str) \\\n",
    "              for application_str in application_strings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b26afe",
   "metadata": {},
   "source": [
    "**Once again, the four cells below will give you long outputs if uncommented.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb565b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24091da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#description_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4708e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#application_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b0d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#application_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bcecc",
   "metadata": {},
   "source": [
    "The next code will take care of the issue I described thusly:\n",
    "> It's annoying me not to have a nice, aligned output for these 2d lists - basically, they're tables. I need to bring in some previous code that takes care of getting stuff printed nice. That will be after the Q&R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from io import StringIO\n",
    "import re\n",
    "\n",
    "def print_2d_list_columns_aligned(list_2d_to_print,\n",
    "                                  joining_delimiter = \",  \",\n",
    "                                  do_output_as_str_and_print = False,\n",
    "                                  do_see_the_guts=False):\n",
    "    ##  The  do_see_the_guts boolean is partly for debugging,\n",
    "    ##+ partly for remembering and teaching how the process\n",
    "    ##+ works.\n",
    "    \n",
    "    ## Make all elements strings - so we can use len()\n",
    "    list_2d_all_strings = \\\n",
    "      [[str(item) for item in row] for row in list_2d_to_print]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_2d_all_strings:\")\n",
    "        print(list_2d_all_strings)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    #  We want to find the max string length for each column\n",
    "    #+ We can basically transpose the 2d_list to get the\n",
    "    #+ content of each column\n",
    "    list_of_column_elems_as_tuples = \\\n",
    "                 [column for column in zip(*list_2d_all_strings)]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_of_column_elems_as_tuples:\")\n",
    "        print(list_of_column_elems_as_tuples)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    ## find the max string length for each tuple (each column)\n",
    "    list_of_max_str_len_by_column = \\\n",
    "      [max([len(strng) for strng in tpl]) \n",
    "        for tpl in list_of_column_elems_as_tuples]\n",
    "    # -v- gives array with elements being each longest string\n",
    "    #[max([strng for strng in tpl], key=len) for tpl in list_of_column_elems_as_tuples]\n",
    "    # -v- 2d array with strings\n",
    "    #[[strng for strng in tpl] for tpl in list_of_column_elems_as_tuples]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_of_max_str_len_by_column:\")\n",
    "        print(list_of_max_str_len_by_column)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    # output_as_str_not_list = False\n",
    "    \n",
    "    # Create a formatter for each row\n",
    "    \n",
    "    #if not output_as_str_not_list:\n",
    "    joining_delimiter = \",\" + joining_delimiter\n",
    "    \n",
    "    fmt_str = \\\n",
    "      joining_delimiter.join('{{:{}}}'.format(max_len) \n",
    "                               for max_len in list_of_max_str_len_by_column)\n",
    "    #if not output_as_str_not_list:\n",
    "    fmt_str = \"[\" + fmt_str + \"],\"\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  fmt_str:\")\n",
    "        print(fmt_str)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    # Get a string for each row, formatted correctly\n",
    "    list_of_formatted_row_strings = \\\n",
    "      [fmt_str.format(*row) for row in list_2d_all_strings]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_of_formatted_row_strings:\")\n",
    "        print(list_of_formatted_row_strings)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    # if output_as_str_not_list:\n",
    "    #     aligned_table_to_return = '\\n'.join(list_of_formatted_row_strings)\n",
    "    #     \n",
    "    #     print(aligned_table_to_return)\n",
    "    #     return aligned_table_to_return\n",
    "    # ##endof:  if output_as_str_not_list\n",
    "    \n",
    "    s = StringIO()\n",
    "    print(*list_of_formatted_row_strings, file=s)\n",
    "    output_table_raw = s.getvalue()\n",
    "    \n",
    "    output_table_raw = re.sub(r\"^\\[\", r\"[[ \", output_table_raw)\n",
    "    output_table_raw = re.sub(r\"\\],$\", r\"]]\", output_table_raw)\n",
    "    #output_table_raw = re.sub(r\"([^ ])\\],\", r\"\\g<1> ],\", output_table_raw)\n",
    "    output_table_raw = re.sub(r\"\\],\", r\" ],\", output_table_raw)\n",
    "    output_table_raw = re.sub(r\",,\", r\" ,\", output_table_raw)\n",
    "    output_table_raw = re.sub(r\"]]\", r\" ]]\", output_table_raw)\n",
    "    \n",
    "    aligned_table_to_return = output_table_raw.replace(r\"], [\", \"],\\n [ \")\n",
    "    \n",
    "    print(aligned_table_to_return)\n",
    "    \n",
    "    if do_output_as_str_and_print:\n",
    "        return aligned_table_to_return\n",
    "    ##endof:  if do_output_as_str_and_print\n",
    "    \n",
    "##endof:  print_2d_list_colunns_aligned(<params>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45976607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##  Uncomment if you want to see the guts of a small example\n",
    "# ##+ that's been hacked into working all right\n",
    "# print_2d_list_columns_aligned([['hey', 7], ['work', 3], ['stupid', 2]], do_see_the_guts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only change this boolean to True if you want to see a lot of output. ##\n",
    "do_print_long_full_version = False\n",
    "\n",
    "if do_print_long_full_version:\n",
    "    #import pprint\n",
    "    \n",
    "    dashes=\"------------------------------------------------------------\"\n",
    "    short_dashes=\"-----\"\n",
    "    \n",
    "    this_d_str_counter = -1 # quick hack for zero-indexed\n",
    "    \n",
    "    for d_word_count_dict in description_word_counts:\n",
    "        this_d_str_counter += 1\n",
    "        \n",
    "        #  I haven't yet written this for several resumes, so\n",
    "        #+ it just compares each description with the first\n",
    "        #+ (or, likely, combined) thing in the resume stuff.\n",
    "        this_a_str_index = 0\n",
    "        a_word_count_dict = application_word_counts[this_a_str_index]\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "        print(dashes)\n",
    "        print(f\" For the job description with index, {this_d_str_counter}\")\n",
    "        print( \" (meaning it's from the file:\")\n",
    "        print(f\"   {local_job_desc_filenames[this_d_str_counter]}),\")\n",
    "        print(short_dashes)\n",
    "        this_d_wdct_items_list = list(d_word_count_dict.items())\n",
    "        this_d_wdct_2d_list = [list (ele) for ele in this_d_wdct_items_list]\n",
    "        #pprint.pprint(this_d_wdct_2d_list)\n",
    "        print_2d_list_columns_aligned(this_d_wdct_2d_list)\n",
    "        print(dashes)\n",
    "        print()\n",
    "        print()\n",
    "        print(dashes)\n",
    "        print( \" For the job application material (résumé, cover letter, etc.)\")\n",
    "        print(f\" with index, {this_a_str_index}\")\n",
    "        print( \"(meaning it's from the file:\")\n",
    "        print(f\"   {local_job_appl_filenames[this_a_str_index]}),\")\n",
    "        print(short_dashes)\n",
    "        this_a_wdct_items_list = list(a_word_count_dict.items())\n",
    "        this_a_wdct_2d_list = [list (ele) for ele in this_a_wdct_items_list]\n",
    "        #pprint.pprint(this_a_wdct_2d_list)\n",
    "        print_2d_list_columns_aligned(this_a_wdct_2d_list)\n",
    "        print(dashes)\n",
    "        print()\n",
    "        print()\n",
    "        print(dashes)\n",
    "        print(dashes)\n",
    "        print()\n",
    "        print()\n",
    "    ##endof:  for d_word_count_dict in description_word_counts\n",
    "##endof:  if do_print_long_full_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_counter = -1 # hack for zero-indexing\n",
    "appl_index   =  0 #  combining application files\n",
    "                  #+ (actually, here, there's only one file)\n",
    "\n",
    "description_items_list = []\n",
    "application_items_list = []\n",
    "\n",
    "this_a_word_count = application_word_counts[appl_index]\n",
    "application_items = list(this_a_word_count.items())\n",
    "application_items_list.append(application_items)\n",
    "\n",
    "for this_d_word_count in description_word_counts:\n",
    "    desc_counter += 1\n",
    "    \n",
    "#     #  I haven't yet written this for several resumes, so\n",
    "#     #+ it just compares each description with the first\n",
    "#     #+ (or, likely, combined) thing in the resume stuff.\n",
    "#     this_a_word_count = application_word_counts[appl_index]\n",
    "    \n",
    "    description_items_list.append(list(this_d_word_count.items()))\n",
    "#     application_items = list(this_a_word_count.items())\n",
    "    \n",
    "    n_words_description = len(description_items_list[desc_counter])\n",
    "    n_words_application = len(application_items_list[appl_index])\n",
    "    \n",
    "    print()\n",
    "    print(\"NOTE THAT THESE ARE THE NUMBERS OF DISTINCT WORDS\")\n",
    "    print()\n",
    "    print(f\" For the job description with index, {desc_counter}\")\n",
    "    print( \" (meaning it's from the file:\")\n",
    "    print(f\"   {local_job_desc_filenames[desc_counter]}),\")\n",
    "    print(\"AND\")\n",
    "    print( \" For the job application material (résumé, cover letter, etc.)\")\n",
    "    print(f\" with index, {appl_index}\")\n",
    "    print( \"(meaning it's from the file:\")\n",
    "    print(f\"   {local_job_appl_filenames[appl_index]}),\")\n",
    "    print()\n",
    "    print(f\"n_words_description = {str(n_words_description)}\")\n",
    "    print(f\"n_words_application = {str(n_words_application)}\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "##endof:  for this_d_word_count in description_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "\n",
    "#<strike>### FOR NOW, CHANGE THIS FOR EACH DESCRIPTION ###</strike>\n",
    "\n",
    "#    ##  Not handling ties differently - whichever shows up first gets the\n",
    "#    ##+ higher ranking\n",
    "#\n",
    "#    # # These start out impossible, so we'll know if we use it without changing\n",
    "#    # this_d_tie_dict_index = -1\n",
    "#    # this_d_tie_value = -1\n",
    "#    # this_a_tie_dict_index = -1\n",
    "#    # this_a_tie_value = -1\n",
    "\n",
    "table_version_desc = None\n",
    "table_version_appl = None\n",
    "table_version_both = None\n",
    "\n",
    "list_of_table_version_desc = []\n",
    "list_of_table_version_appl = []\n",
    "list_of_table_version_both = []\n",
    "\n",
    "# ##  We will do the following for headers\n",
    "# table_version_desc_start = [[\"desc_word\", \"desc_cnt\", \"desc_rank\"],]\n",
    "# table_version_appl_start = [[\"appl_word\", \"appl_cnt\", \"appl_rank\"],]\n",
    "# table_version_both_start = [[\"rank\", \"desc_word\", \"desc_cnt\", \n",
    "#                              \"appl_word\", \"appl_cnt\"],\n",
    "#                            ]\n",
    "#### Dang immutable, pass-by-reference Python stuff. : )\n",
    "\n",
    "\n",
    "for this_description_items in description_items_list:\n",
    "    if table_version_desc is not None:\n",
    "        table_version_desc.clear()\n",
    "    if table_version_appl is not None:\n",
    "        table_version_appl.clear()\n",
    "    if table_version_both is not None:\n",
    "        table_version_both.clear()\n",
    "    \n",
    "    table_version_desc = [[\"desc_word\", \"desc_cnt\", \"desc_rank\"],]\n",
    "    table_version_appl = [[\"appl_word\", \"appl_cnt\", \"appl_rank\"],]\n",
    "    table_version_both = [[\"rank\", \"desc_word\", \"desc_cnt\", \n",
    "                           \"appl_word\", \"appl_cnt\"],\n",
    "                         ]\n",
    "    \n",
    "    for this_idx in range(max(len(this_description_items),\n",
    "                              len(application_items)\n",
    "                             ) - 1\n",
    "                          ):\n",
    "        \n",
    "        this_rank = this_idx + 1\n",
    "        \n",
    "        this_description_word  = \"\"\n",
    "        this_description_count = \"\"\n",
    "        this_description_rank  = \"\"\n",
    "        \n",
    "        if this_idx < len(this_description_items) - 1:\n",
    "            try:\n",
    "                this_description_word  = this_description_items[this_idx+1][0]\n",
    "            except IndexError as ie:\n",
    "                print(\"OTHER ERROR desc word!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"this_idx: {str(this_idx)}\", file=sys.stderr)\n",
    "                print(f\"this_rank: {str(this_rank)}\", file=sys.stderr)\n",
    "                print(f\"len(this_description_items): {str(len(this_description_items))}\", file=sys.stderr)\n",
    "                end_of_data_bool_try = ( this_idx <= len(this_description_items) )\n",
    "                print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/catch/finally\n",
    "\n",
    "            try:\n",
    "                this_description_count = this_description_items[this_idx+1][1]\n",
    "            except IndexError as ie:\n",
    "                print(\"OTHER ERROR desc count!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"this_idx: {str(this_idx)}\", file=sys.stderr)\n",
    "                print(f\"this_rank: {str(this_rank)}\", file=sys.stderr)\n",
    "                print(f\"len(this_description_items): {str(len(this_description_items))}\", file=sys.stderr)\n",
    "                end_of_data_bool_try = ( this_idx <= len(this_description_items) )\n",
    "                print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/catch/finally\n",
    "\n",
    "            this_description_rank  = this_rank\n",
    "        else:\n",
    "            this_description_word  = \"-- N/A --\"\n",
    "            this_description_count = \"-- N/A --\"\n",
    "            this_description_rank  = \"-- N/A --\"\n",
    "        ##endof:  if/else this_idx < len(description_items)\n",
    "        \n",
    "        this_application_word  = \"\"\n",
    "        this_application_count = \"\"\n",
    "        this_application_rank  = \"\"\n",
    "        \n",
    "        if this_idx < len(application_items) - 1:\n",
    "            try:\n",
    "                this_application_word  = application_items[this_idx+1][0]\n",
    "            except IndexError as ie:\n",
    "                print(\"OTHER ERROR appl word!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"this_idx: {str(this_idx)}\", file=sys.stderr)\n",
    "                print(f\"this_rank: {str(this_rank)}\", file=sys.stderr)\n",
    "                print(f\"len(application_items): {str(len(application_items))}\", file=sys.stderr)\n",
    "                end_of_data_bool_try = ( this_idx <= len(application_items) )\n",
    "                print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/catch/finally\n",
    "\n",
    "            try:\n",
    "                this_application_count = application_items[this_idx+1][1]\n",
    "            except IndexError as ie:\n",
    "                print(\"OTHER ERROR appl count!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"this_idx: {str(this_idx)}\", file=sys.stderr)\n",
    "                print(f\"this_rank: {str(this_rank)}\", file=sys.stderr)\n",
    "                print(f\"len(application_items): {str(len(application_items))}\", file=sys.stderr)\n",
    "                end_of_data_bool_try = ( this_idx <= len(application_items) )\n",
    "                print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/catch/finally\n",
    "\n",
    "            this_application_rank  = this_rank\n",
    "        else:\n",
    "            this_application_word  = \"-- N/A --\"\n",
    "            this_application_count = \"-- N/A --\"\n",
    "            this_application_rank  = \"-- N/A --\"\n",
    "        ##endof:  if/else this_idx < len(application_items)\n",
    "\n",
    "        if this_description_word != \"-- N/A --\":\n",
    "            try:\n",
    "                table_version_desc.append([this_description_word, \n",
    "                                           this_description_count, \n",
    "                                           this_rank]\n",
    "                                         )\n",
    "            except IndexError as ie:\n",
    "                print(\"ERROR desc!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"this_idx: {str(this_idx)}\", file=sys.stderr)\n",
    "                print(f\"this_rank: {str(this_rank)}\", file=sys.stderr)\n",
    "                print(f\"this_description_word: {str(this_description_word)}\", file=sys.stderr)\n",
    "                print(f\"this_description_count: {str(this_description_count)}\", file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/except/finally\n",
    "\n",
    "        ##endof:  if this_description_word != \"-- N/A --\"\n",
    "\n",
    "        if this_application_word != \"-- N/A --\":\n",
    "            try:\n",
    "                table_version_appl.append([this_application_word, \n",
    "                                           this_application_count, \n",
    "                                           this_rank\n",
    "                                          ]\n",
    "                                         )\n",
    "            except IndexError as ie:\n",
    "                print(\"ERROR appl!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"this_idx: {this_idx}\", file=sys.stderr)\n",
    "                print(f\"this_rank: {str(this_rank)}\", file=sys.stderr)\n",
    "                print(f\"this_application_word: {str(this_application_word)}\", file=sys.stderr)\n",
    "                print(f\"this_application_count: {str(this_application_count)}\", file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/except/finally\n",
    "\n",
    "        ##endof:  if this_application_word != \"-- N/A --\"\n",
    "\n",
    "        try:\n",
    "            table_version_both.append([this_rank, \n",
    "                                       this_description_word, \n",
    "                                       this_description_count,\n",
    "                                       this_application_word, \n",
    "                                       this_application_count\n",
    "                                      ]\n",
    "                                     )\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR both!\", file=sys.stderr)\n",
    "            print(str(ie), file=sys.stderr)\n",
    "            print(f\"this_idx: {this_idx}\", file=sys.stderr)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stderr)\n",
    "            print(f\"this_description_word: {str(this_description_word)}\", file=sys.stderr)\n",
    "            print(f\"this_description_count: {str(this_description_count)}\", file=sys.stderr)\n",
    "            print(f\"this_application_word: {str(this_application_word)}\", file=sys.stderr)\n",
    "            print(f\"this_application_count: {str(this_application_count)}\", file=sys.stderr)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/except/finally\n",
    "\n",
    "    ##endof:  for this_idx in max(<n_desc_words>, <n_appl_words>)\n",
    "    \n",
    "    deep_desc = copy.deepcopy(table_version_desc)\n",
    "    deep_appl = copy.deepcopy(table_version_appl)\n",
    "    deep_both = copy.deepcopy(table_version_both)\n",
    "    \n",
    "    list_of_table_version_desc.append(deep_desc)\n",
    "    list_of_table_version_appl.append(deep_appl)\n",
    "    list_of_table_version_both.append(deep_both)\n",
    "    \n",
    "##endof:  for this_description_items in description_items_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036a6d48",
   "metadata": {},
   "source": [
    "**The next 3 cells are other cells for which, if the code be uncommented, you will get a lot of output.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f321c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pprint; pprint.pprint(list_of_table_version_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f185481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pprint; pprint.pprint(list_of_table_version_appl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b73c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pprint; pprint.pprint(list_of_table_version_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "##  Set up the display the first n_lines_to_display \n",
    "##+ of the tables, nicely\n",
    "\n",
    "#### This next one is the one you might change\n",
    "n_lines_to_display_orig = 25\n",
    "\n",
    "n_header_lines = 1\n",
    "n_lines_to_display = n_lines_to_display_orig\n",
    "n_lines_to_display_desc = n_lines_to_display_orig\n",
    "n_lines_to_display_appl = n_lines_to_display_orig\n",
    "\n",
    "## Here is where the code for doing it with lists of strings comes\n",
    "\n",
    "display_table_desc = None\n",
    "display_table_appl = None\n",
    "display_table_both = None\n",
    "\n",
    "list_of_display_table_desc = []\n",
    "list_of_display_table_appl = []\n",
    "list_of_display_table_both = []\n",
    "\n",
    "for desc_fname_idx_for_now in range(len(list_of_table_version_desc)):\n",
    "    #  find out which one is longer. Make that the length of the\n",
    "    #+ display_table_both\n",
    "    do_cut_down_desc = ( n_lines_to_display_orig >\n",
    "                            len(list_of_table_version_desc[desc_fname_idx_for_now]) \n",
    "    )\n",
    "    if do_cut_down_desc:\n",
    "        n_lines_to_display_desc = len(list_of_table_version_desc[desc_fname_idx_for_now])\n",
    "    ##endof:  if do_cut_down_desc\n",
    "    \n",
    "    do_cut_down_appl = ( n_lines_to_display_orig >\n",
    "                            len(list_of_table_version_appl[desc_fname_idx_for_now]) \n",
    "    )\n",
    "    if do_cut_down_appl:\n",
    "        n_lines_to_display_appl = len(list_of_table_version_appl[desc_fname_idx_for_now])\n",
    "    ##endof:  if do_cut_down_appl\n",
    "    \n",
    "    # start with a new, immutable, pass-by-reference table\n",
    "    if display_table_desc is not None:\n",
    "        display_table_desc.clear()\n",
    "    if display_table_appl is not None:\n",
    "        display_table_appl.clear()\n",
    "    if display_table_both is not None:\n",
    "        display_table_both.clear()\n",
    "    \n",
    "    # get headers\n",
    "    display_table_desc = [list_of_table_version_desc[desc_fname_idx_for_now][0]]\n",
    "    display_table_appl = [list_of_table_version_appl[desc_fname_idx_for_now][0]]\n",
    "        # made copies to make it easier\n",
    "    display_table_both = [list_of_table_version_both[desc_fname_idx_for_now][0]]\n",
    "    \n",
    "    if ( len(list_of_table_version_desc[desc_fname_idx_for_now]) - n_header_lines < n_lines_to_display or\n",
    "         len(list_of_table_version_appl[desc_fname_idx_for_now]) - n_header_lines < n_lines_to_display\n",
    "    ):\n",
    "        n_lines_to_display = min(len(list_of_table_version_desc[desc_fname_idx_for_now]) - n_header_lines,\n",
    "                                 len(list_of_table_version_appl[desc_fname_idx_for_now]) - n_header_lines)\n",
    "    ##endof:  if <n_lines_conditions>\n",
    "    \n",
    "    \n",
    "    for table_idx in range(n_header_lines, \n",
    "                           n_lines_to_display_orig + n_header_lines):\n",
    "        if table_idx - n_header_lines < n_lines_to_display_desc:\n",
    "            try:\n",
    "                display_table_desc.append(list_of_table_version_desc[desc_fname_idx_for_now][table_idx])\n",
    "            except IndexError as ie:\n",
    "                print(\"ERROR display_table_desc!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"table_idx: {table_idx}\", file=sys.stderr)\n",
    "                print(f\"n_lines_to_display_desc: {n_lines_to_display_desc}\", file=sys.stderr)\n",
    "                print(f\"len(list_of_table_version_desc[{desc_fname_idx_for_now}]):\" + \\\n",
    "                      f\" {len(list_of_table_version_desc[desc_fname_idx_for_now])}\", \n",
    "                      file=sys.stderr)\n",
    "            except Error as e:\n",
    "                print(\"DIFFERENT ERROR display_table_desc!\", file=sys.stderr)\n",
    "                print(str(e), file=sys.stderr)\n",
    "                print(f\"table_idx: {table_idx}\", file=sys.stderr)\n",
    "                print(f\"n_lines_to_display_desc: {n_lines_to_display_desc}\", file=sys.stderr)\n",
    "                print(f\"len(list_of_table_version_desc[{desc_fname_idx_for_now}]):\" + \\\n",
    "                      f\" {len(list_of_table_version_desc[desc_fname_idx_for_now])}\", \n",
    "                      file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/catch/finally\n",
    "        ##endof:  if table_idx - n_header_lines < n_lines_to_display_desc\n",
    "        \n",
    "        if table_idx - n_header_lines < n_lines_to_display_appl:\n",
    "            try:\n",
    "                display_table_appl.append(list_of_table_version_appl[desc_fname_idx_for_now][table_idx])\n",
    "            except IndexError as ie:\n",
    "                print(\"ERROR display_table_appl!\", file=sys.stderr)\n",
    "                print(str(ie), file=sys.stderr)\n",
    "                print(f\"table_idx: {table_idx}\", file=sys.stderr)\n",
    "                print(f\"n_lines_to_display_appl: {n_lines_to_display_appl}\", file=sys.stderr)\n",
    "                print(f\"len(list_of_table_version_appl[{desc_fname_idx_for_now}]):\" + \\\n",
    "                      f\" {len(list_of_table_version_appl[desc_fname_idx_for_now])}\", \n",
    "                      file=sys.stderr)\n",
    "            except Error as e:\n",
    "                print(\"DIFFERENT ERROR display_table_appl!\", file=sys.stderr)\n",
    "                print(str(e), file=sys.stderr)\n",
    "                print(f\"table_idx: {table_idx}\", file=sys.stderr)\n",
    "                print(f\"n_lines_to_display_appl: {n_lines_to_display_appl}\", file=sys.stderr)\n",
    "                print(f\"len(list_of_table_version_appl[{desc_fname_idx_for_now}]):\" + \\\n",
    "                      f\" {len(list_of_table_version_appl[desc_fname_idx_for_now])}\", \n",
    "                      file=sys.stderr)\n",
    "            finally:\n",
    "                pass\n",
    "            ##endof:  try/catch/finally\n",
    "        ##endof:  if table_idx - n_header_lines < n_lines_to_display_appl\n",
    "        \n",
    "        try:\n",
    "            display_table_both.append(list_of_table_version_both[0][table_idx])\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR display_table_both!\", file=sys.stderr)\n",
    "            print(str(ie), file=sys.stderr)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stderr)\n",
    "            print(f\"len(list_of_table_version_both[{desc_fname_idx_for_now}]):\" + \\\n",
    "                  f\" {len(list_of_table_version_both[0])}\", \n",
    "                  file=sys.stderr)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "    ##endof:  for idx in range(<n_lines stuff>)\n",
    "    \n",
    "    ####  Dang immutable, pass-by-reference Python stuff. : )\n",
    "    ####+ I'm used to more C-style, but I'm getting better.\n",
    "    deep_display_desc = copy.deepcopy(display_table_desc)\n",
    "    deep_display_appl = copy.deepcopy(display_table_appl)\n",
    "    deep_display_both = copy.deepcopy(display_table_both)\n",
    "    \n",
    "    list_of_display_table_desc.append(deep_display_desc)\n",
    "    list_of_display_table_appl.append(deep_display_appl)\n",
    "    list_of_display_table_both.append(deep_display_both)\n",
    "##endof:  for desc_fname_idx_for_now in range(range(list_of_table_version_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f830db",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_dashes = \"--------------------------------------------------------------\"\n",
    "short_dashes = \"-----\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for desc_fname_idx_for_now in range(len(list_of_display_table_desc)):\n",
    "    print()\n",
    "    print(long_dashes)\n",
    "    print(f\"len(list_of_display_table_desc[{desc_fname_idx_for_now}]):\"+ \\\n",
    "          f\" {len(list_of_table_version_desc[desc_fname_idx_for_now])}\")\n",
    "    print(f\"len(display_table_desc): {len(display_table_desc)}\")\n",
    "    print()\n",
    "    print(f\"len(list_of_display_table_appl[{desc_fname_idx_for_now}]):\" + \\\n",
    "          f\" {len(list_of_display_table_appl[desc_fname_idx_for_now])}\")\n",
    "    print(f\"len(display_table_appl): {len(display_table_appl)}\")\n",
    "    print()\n",
    "    print(f\"len(list_of_display_table_both[{desc_fname_idx_for_now}]):\" + \\\n",
    "          f\" {len(list_of_display_table_both[desc_fname_idx_for_now])}\")\n",
    "    print(f\"len(display_table_both): {len(display_table_both)}\")\n",
    "    print(long_dashes)\n",
    "    print()\n",
    "##endof:  for desc_fname_idx_for_now in range(len(list_of_table_version_desc)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae6913",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import pprint\n",
    "\n",
    "desc_counter = -1 # hack for zero-indexed\n",
    "for this_desc_disp_table in list_of_display_table_desc:\n",
    "    print()\n",
    "    print(long_dashes + short_dashes)\n",
    "    desc_counter += 1\n",
    "    print( \"JOB DESCRIPTION (TOP 25)\")\n",
    "    print(f\"  from file: {local_job_desc_filenames[desc_counter]}\")\n",
    "    print(short_dashes)\n",
    "    #pprint.pprint(display_table_desc)\n",
    "    print_2d_list_columns_aligned(this_desc_disp_table)\n",
    "    print()\n",
    "    print(long_dashes)\n",
    "    print()\n",
    "##endof:  for this_desc_disp_table in list_of_display_table_desc\n",
    "\n",
    "print(long_dashes + 2*short_dashes)\n",
    "print(long_dashes + 2*short_dashes)\n",
    "\n",
    "for this_appl_disp_table in list_of_display_table_appl:\n",
    "    print()\n",
    "    print(long_dashes)\n",
    "    print(\"JOB APPLICATION STUFF - RéSUMé AND COVER LETTER (TOP 25)\")\n",
    "    print(short_dashes)\n",
    "    #pprint.pprint(display_table_appl)\n",
    "    print_2d_list_columns_aligned(this_appl_disp_table)\n",
    "    print()\n",
    "    print(long_dashes)\n",
    "    print()\n",
    "##endof:  for this_appl_disp_table in list_of_display_table_appl\n",
    "\n",
    "print(long_dashes + 2*short_dashes)\n",
    "print(long_dashes + 2*short_dashes)\n",
    "\n",
    "other_desc_counter = -1 # hack for zero-indexed\n",
    "for this_both_disp_table in list_of_display_table_both:\n",
    "    print()\n",
    "    print(long_dashes)\n",
    "    other_desc_counter += 1\n",
    "    print(\"COMPARISON OF DESCRIPTION AND APPLICATION (TOP 25)\")\n",
    "    print(f\"  from file: {local_job_desc_filenames[other_desc_counter]}\")\n",
    "    print(short_dashes)\n",
    "    #pprint.pprint(display_table_both)\n",
    "    print_2d_list_columns_aligned(this_both_disp_table)\n",
    "    print()\n",
    "    print(long_dashes)\n",
    "##endof:  for this_both_disp_table in list_of_table_version_both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c937df8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_word_at_rank(this_rank = 1, \n",
    "                                 this_desc_fname_idx=0,\n",
    "                                 do_print_details=False\n",
    "                                ):\n",
    "    this_idx = this_rank # the header is index 0\n",
    "    this_table_to_use = \\\n",
    "      list_of_display_table_desc[this_desc_fname_idx]\n",
    "    this_word = this_table_to_use[this_rank][0]\n",
    "    if do_print_details:\n",
    "        print()\n",
    "        print(f\"  The job description word at rank {this_rank},\")\n",
    "        print( \n",
    "          ( \"  from file:\"\n",
    "           f\" '{local_job_desc_filenames[this_desc_fname_idx]}',\"\n",
    "          )\n",
    ")\n",
    "        print(f\"  is '{this_word}'.\")\n",
    "        print()\n",
    "    ##endof: if do_print_details\n",
    "    \n",
    "    return this_word\n",
    "##endof:  get_description_word_at_rank(<params>)\n",
    "\n",
    "def get_application_word_at_rank(this_rank = 1, \n",
    "                                 do_print_details=False\n",
    "                                ):\n",
    "    this_idx = this_rank # the header is index 0\n",
    "    this_appl_fname_idx=0\n",
    "    this_table_to_use = \\\n",
    "      list_of_display_table_appl[this_appl_fname_idx]\n",
    "    this_word = this_table_to_use[this_rank][0]\n",
    "    if do_print_details:\n",
    "        print()\n",
    "        print(f\"  The job application word at rank {this_rank},\")\n",
    "        print( \n",
    "          ( \"  from file:\"\n",
    "           f\" '{local_job_appl_filenames[this_appl_fname_idx]}',\"\n",
    "          )\n",
    ")\n",
    "        print(f\"  is '{this_word}'.\")\n",
    "        print()\n",
    "    ##endof: if do_print_details\n",
    "    \n",
    "    return this_word\n",
    "##endof:  get_description_word_at_rank(<params>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9d5f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_description_word_at_rank(1, do_print_details=True);\n",
    "get_application_word_at_rank(1, do_print_details=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832f5e7",
   "metadata": {},
   "source": [
    "### Seems like a good time to look at comparisons\n",
    "\n",
    "#### Between the résumé and the different job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300922dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_in_both_display_lists(word_to_find,\n",
    "                                    display_list_1_description,\n",
    "                                    display_list_2_application,\n",
    "                                    name_of_display_list_1=None,\n",
    "                                    name_of_display_list_2=None,\n",
    "                                    do_print_details=False\n",
    "                                   ):\n",
    "    index_count_1 = 0 # skip header\n",
    "    index_for_found_in_1 = 0\n",
    "    \n",
    "    loop_display_list = display_list_1_description\n",
    "    \n",
    "    word_found_in_1 = False\n",
    "    for my_entry_1 in display_list_1_description:\n",
    "        index_count_1 += 1\n",
    "        print(f\"index_count_1: {index_count_1}\")\n",
    "        print(f\"my_entry_1: {my_entry_1}\")\n",
    "        print(f\"word_to_find: {word_to_find}\")\n",
    "        print(f\"my_entry_1 == word_to_find: {my_entry_1 == word_to_find}\")\n",
    "        input(\"figure it out then press enter\")\n",
    "        if my_entry_1 == word_to_find:\n",
    "            print(\"we got the match\")\n",
    "            word_found_in_1 = True\n",
    "            index_for_found_in_1 = index_count_1\n",
    "            print(f\"word_found_in_1: {word_found_in_1}\")\n",
    "            print(f\"index_for_found_in_1: {index_for_found_in_1}\")\n",
    "            break\n",
    "        ##endof:  if my_entry_1 == word_to_find\n",
    "    ##endof:  for my_entry_1 in display_list_1\n",
    "    \n",
    "    print()\n",
    "    print(\"out of loop 1\")\n",
    "    input(\"press enter\")\n",
    "    print()\n",
    "    \n",
    "    index_count_2 = 0 # skip header\n",
    "    index_for_found_in_2 = -1\n",
    "    word_found_in_2 = False\n",
    "    for my_entry_2 in display_list_2_application:\n",
    "        index_count_2 += 1\n",
    "        print(f\"my_entry_2: {my_entry_2}\")\n",
    "        print(f\"word_to_find: {word_to_find}\")\n",
    "        print(f\"my_entry_2 == word_to_find: {my_entry_2 == word_to_find}\")\n",
    "        input(\"figure it out then press enter\")\n",
    "        if my_entry_2 == word_to_find:\n",
    "            word_found_in_2 = True\n",
    "            index_for_found_in_2 = index_count_2\n",
    "            break\n",
    "        ##endof:  if my_entry_2 == word_to_find\n",
    "    ##endof:  for my_entry_1 in display_list_1\n",
    "    \n",
    "    to_return_found_1 = None\n",
    "    \n",
    "    if word_found_in_1:\n",
    "        to_return_found_1 = index_for_found_in_1 - 1\n",
    "        if do_print_details:\n",
    "            print()\n",
    "            print(f\"The word, {word_to_find}, has rank, {to_return_found_1},\")\n",
    "            if name_of_display_list_1 is not None:\n",
    "                print(f\"in list, {name_of_display_list_1}.\")\n",
    "            #endof:  if name_of_display_list_1 is not None\n",
    "        ##endof:  if do_print_details\n",
    "    ##endof:  if word_found_in_1\n",
    "    \n",
    "    to_return_found_2 = None\n",
    "    \n",
    "    if word_found_in_2:\n",
    "        to_return_found_2 = index_for_found_in_2 - 1\n",
    "        if do_print_details:\n",
    "            print()\n",
    "            print(f\"The word, {word_to_find}, has rank, {to_return_found_2},\")\n",
    "            if name_of_display_list_2 is not None:\n",
    "                print(f\"in list, {name_of_display_list_2}.\")\n",
    "            ##endof:  if name_of_display_list_2 is not None\n",
    "        ##endof:  if do_print_details\n",
    "    ##endof:  if word_found_in_2\n",
    "    \n",
    "    return to_return_found_1, to_return_found_2\n",
    "    \n",
    "##endof:  find_word_in_both_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2590f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Next line only for Jupyter notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def get_freq_histo_specific(word_count_ordered_dict_1,\n",
    "#                           word_count_ordered_dict_2,\n",
    "                            rank_index_1 = 1,\n",
    "                            n_surrounding_words = 3,\n",
    "                            do_show_word_and_count_lists=False,\n",
    "                            ax1=None,\n",
    "#                           ,ax2=None\n",
    "                            ylim_bottom_val=None,\n",
    "                            ylim_top_val=None\n",
    "                           ):\n",
    "    '''\n",
    "    @return  an axis from matplotlab (with the object - histogram - in it)\n",
    "    '''\n",
    "    \n",
    "    if ax1 is None:\n",
    "        fig = plt.figure(figsize=(10, 3))\n",
    "        ax1 = fig.add_subplot(111)\n",
    "#       ax2 = fig.add_subplot(121)\n",
    "    \n",
    "    counts_pre = list(word_count_ordered_dict_1.values())\n",
    "    words_pre  = list(word_count_ordered_dict_1.keys())\n",
    "    \n",
    "    #word_1, count_1 = word_count_ordered_dict_1[rank_index_1]\n",
    "    \n",
    "    #highest_rank_index = -1\n",
    "    \n",
    "    # Pad the list with zero-count and empty-set characters\n",
    "    len_lists = 2 * n_surrounding_words + 1\n",
    "    counts = [0] * len_lists\n",
    "    words  = [\"\\u2205\"] * len_lists\n",
    "    \n",
    "    #  Fill anything with a valid index with the corresponding\n",
    "    #+ word/count\n",
    "    \n",
    "    current_output_index = -1\n",
    "    \n",
    "    for i in range(rank_index_1 - n_surrounding_words,\n",
    "                   rank_index_1 + n_surrounding_words + 1\n",
    "                  ):\n",
    "        current_output_index += 1\n",
    "        if i < 1:\n",
    "            pass\n",
    "        else:\n",
    "            counts[current_output_index] = counts_pre[i]\n",
    "            words[current_output_index] = words_pre[i]\n",
    "        ##endof:  if/else i < 1\n",
    "    ##endof:  for i in range\n",
    "    \n",
    "    ## making sure things are working\n",
    "    if do_show_word_and_count_lists:\n",
    "        print(f\"counts: {counts}\")\n",
    "        print(f\"words:  {words}\")\n",
    "    ##endof:  if do_show_word_and_count_lists\n",
    "    \n",
    "    #input(\"Press [Enter] to continue.\")\n",
    "    \n",
    "    x_words_coords = np.arange(len(words))\n",
    "    ax1.bar(x_words_coords, counts, align='center')\n",
    "    \n",
    "    ax1.set_xticks(x_words_coords)\n",
    "    ax1.set_xticklabels(words, rotation=45, ha='right')\n",
    "    \n",
    "    ax1.set_ylim(ylim_bottom_val, ylim_top_val)\n",
    "    \n",
    "##endof:  get_freq_histo_specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe1780",
   "metadata": {},
   "source": [
    "<strike>Below will be code to look for the top 25 (maybe less, maybe more) description words. I'll go through every word that appears 3 times, and I won't include any that appear only twice or once. I'll see where they appear in my résumé list.</strike>\n",
    "\n",
    "<strike>This will be easily automated and done with a for loop or list comprehension. However, I want to look at some things more manually - that should make the automated stuff better.</strike>\n",
    "\n",
    "I'm going to make this part more of a look-for-each-word thing. The display is too busy to show each word for each file.\n",
    "\n",
    "I have a few improvements that would be good, soon:<br/>\n",
    "  @TODO : get rid of one letter words<br/>\n",
    "  @TODO : look through the rest of the list to get rid of junk\n",
    "\n",
    "I want to match two histograms for this stuff, with e.g. the job description's word and (up to) 3 (or 4 or 5 or 6 or 2 or 1 or ...) words more frequent and (up to) 3 words less frequent. I'm going to bring up a picture of the histograms for my brainstorming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7d33e",
   "metadata": {},
   "source": [
    "### Here are the specific word-rank comparison histograms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf7310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_for_choices = f\"Choices are any of: {list(range(len(local_job_desc_filenames)))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b583fcf",
   "metadata": {},
   "source": [
    "####  For this section, we have calculated everything, but show just two files being compared\n",
    "\n",
    "Well, when we get to the compare-all-top-25 histograms, we'll show all the comparisons.\n",
    "\n",
    "For the comparisons of the top-ranked words, just two files at a time\n",
    "\n",
    "Another thing, to keep this Quick and Reckless (not spending too much time), I'm dispensing with my cherished 80 characters per line. `: (`\n",
    "\n",
    "**You can change the `desc_fname_idx_to_show` to any of the numbers in the next output ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str_for_choices.replace(r\"[\", r\"{\").replace(r\"]\", r\"}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd9eee5",
   "metadata": {},
   "source": [
    "Output was most recently\n",
    "\n",
    "Choices are any of: `{0, 1, 2, 3}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f8c5ce",
   "metadata": {},
   "source": [
    "... **to see results for a specific job description.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for my_desc_index in range(len(local_job_desc_filenames)):\n",
    "    print(f\"Choice {my_desc_index} : {local_job_desc_filenames[my_desc_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162e67ab",
   "metadata": {},
   "source": [
    "### ... for your choice of job description and word/word rank\n",
    "\n",
    "(rank in the job description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9cda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your choice:\n",
    "desc_fname_idx_to_show = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02946142",
   "metadata": {},
   "source": [
    "### Now we can continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257f482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_word_rank_in_desc = 1\n",
    "this_desc_idx = top_word_rank_in_desc\n",
    "\n",
    "this_corresponding_word = get_description_word_at_rank(top_word_rank_in_desc)\n",
    "\n",
    "rank_desc, _ = find_word_in_both_display_lists(\n",
    "                this_corresponding_word,\n",
    "                description_word_counts[desc_fname_idx_to_show],\n",
    "                application_word_counts[0],\n",
    "                   #  we only have one table - \n",
    "                   #+ it's at any legal index;\n",
    "                   #+ let's choose 0\n",
    "                name_of_display_list_1 = f\"description_word_counts[{this_desc_idx}]\",\n",
    "                name_of_display_list_2 = \"application_word_counts[0]\"\n",
    ")\n",
    "\n",
    "fig_filename_desc = \"\"\n",
    "\n",
    "if rank_desc is None:\n",
    "    import matplotlib.image as mpimg\n",
    "    fig_filename_desc = \"description_word_not_found.png\"\n",
    "    img = mpimg.imread(fig_filename_desc)\n",
    "    imgplot = plt.imshow(img)\n",
    "##endof:  if rank_desc\n",
    "else:\n",
    "    get_freq_histo_specific(\n",
    "            description_word_counts[desc_fname_idx_to_show],\n",
    "            rank_index_1=top_word_rank_in_desc,\n",
    "            n_surrounding_words=3,\n",
    "            do_show_word_and_count_lists=False,\n",
    "            ylim_top_val=10)\n",
    "    \n",
    "    fig_filename_desc = (\n",
    "            f\"description_word_rank_{top_word_rank_in_desc}_\"\n",
    "            f\"desc_{desc_fname_idx_to_show}.png\"\n",
    "    )\n",
    "\n",
    "    title_for_desc = (f\"Word frequency rank ({rank_desc}) and surrounding context in \"\n",
    "                      f\"job description for the word, {this_corresponding_word}\"\n",
    "                 )\n",
    "    plt.title(title_for_desc)\n",
    "\n",
    "    plt.savefig(fig_filename_desc,\n",
    "                bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "##endof:  if/else rank_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25cb52f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "this_corresponding_word = get_description_word_at_rank(top_word_rank_in_desc)\n",
    "this_desc_idx = desc_fname_idx_to_show\n",
    "word_indexes = find_word_in_both_display_lists(\n",
    "                 this_corresponding_word,\n",
    "                 description_word_counts[desc_fname_idx_to_show],\n",
    "                 application_word_counts[0],\n",
    "                   #  we only have one table - \n",
    "                   #+ it's at any legal index;\n",
    "                   #+ let's choose 0\n",
    "                 name_of_display_list_1 = f\"description_word_counts[{this_desc_idx}]\",\n",
    "                 name_of_display_list_2 = \"application_word_counts[0]\"                \n",
    ")\n",
    "\n",
    "print()\n",
    "print( (\"(rank in description, rank in application) for the word,\"\n",
    "        f\" '{this_corresponding_word}': {word_indexes}\"\n",
    "       )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3a73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "this_word = this_corresponding_word\n",
    "this_desc_idx = desc_fname_idx_to_show\n",
    "word_indexes = find_word_in_both_display_lists(\n",
    "                 this_word,\n",
    "                 description_word_counts[desc_fname_idx_to_show],\n",
    "                 application_word_counts[0],\n",
    "                   #  we only have one table - \n",
    "                   #+ it's at any legal index;\n",
    "                   #+ let's choose 0\n",
    "                 name_of_display_list_1 = f\"description_word_counts[{this_desc_idx}]\",\n",
    "                 name_of_display_list_2 = \"application_word_counts[0]\"\n",
    "                 \n",
    ")\n",
    "\n",
    "print()\n",
    "print( ( \"(rank in description, rank in application) for the word,\"\n",
    "        f\" '{this_corresponding_word}': {word_indexes}\"\n",
    "       )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448a1fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, rank_appl = find_word_in_both_display_lists(\n",
    "        this_corresponding_word,\n",
    "        description_word_counts[desc_fname_idx_to_show],\n",
    "        application_word_counts[0],\n",
    "           #  we only have one table - \n",
    "           #+ it's at any legal index;\n",
    "           #+ let's choose 0\n",
    "        name_of_display_list_1 = f\"description_word_counts[{this_desc_idx}]\",\n",
    "        name_of_display_list_2 = \"application_word_counts[0]\"\n",
    ")\n",
    "\n",
    "fig_filename_appl = \"\"\n",
    "\n",
    "if rank_appl is None:\n",
    "    import matplotlib.image as mpimg\n",
    "    fig_filename_appl = \"application_word_not_found.png\"\n",
    "    img = mpimg.imread(fig_filename_appl)\n",
    "    imgplot = plt.imshow(img)\n",
    "##endof:  if rank_desc\n",
    "else:\n",
    "    corresponding_index = rank_appl\n",
    "    \n",
    "    get_freq_histo_specific(application_word_counts[0],\n",
    "                        rank_index_1=corresponding_index,\n",
    "                        n_surrounding_words=3,\n",
    "                        do_show_word_and_count_lists=False,\n",
    "                        ylim_top_val=10)\n",
    "\n",
    "    fig_filename_appl = (f\"application_word_rank_{corresponding_index}_\"\n",
    "                         f\"desc_{desc_fname_idx_to_show}.png\"\n",
    "                        )\n",
    "\n",
    "    title_for_appl = (f\"Word frequency rank ({rank_appl}) and surrounding context in \"\n",
    "                      f\"job application for the word, {this_corresponding_word}\"\n",
    "                     )\n",
    "    plt.title(title_for_appl)\n",
    "\n",
    "    plt.savefig(fig_filename_appl,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "##endof:  ##endof:  if/else rank_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  img src values for the two images:\")\n",
    "print(f'\"{fig_filename_desc}\"')\n",
    "print(f'\"{fig_filename_appl}\"')\n",
    "\n",
    "print()\n",
    "print(\"  img alt values for the two images:\")\n",
    "alt_text_1 = (f'\"Histogram for the word, {this_corresponding_word}, in '\n",
    "               'the job description text\"'\n",
    "             )\n",
    "alt_text_2 = (f'\"Histogram for the word, {this_corresponding_word}, in '\n",
    "               'the job application text\"'\n",
    "             )\n",
    "print(alt_text_1)\n",
    "print(alt_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99f8fb",
   "metadata": {},
   "source": [
    "### Change the img src values and img alt values, then see the histograms together\n",
    "\n",
    "You might need to double-click on the image to get the html source.\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"description_word_rank_1_desc_0.png\"\n",
    "       alt=\"Histogram for the word, work, in the job description text\"\n",
    "       width=\"auto\">\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"application_word_rank_20_desc_0.png\"\n",
    "       alt=\"Histogram for the word, work, in the job application text\"\n",
    "       width=\"auto\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e098db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Code to look for the top 25 (probably more) application\n",
    "#+ (résumé) words (down to appearing 3 times -- not\n",
    "#+ including 2) and see where they appear in the job\n",
    "#+ descriptions (if at all.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482669e",
   "metadata": {},
   "source": [
    "## Time for top-25 histograms (or whatever the discretized version is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5092550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Next line only for Jupyter notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def get_histo_from_freq_dict(word_count_ordered_dict,\n",
    "                             n_top_words = 25,\n",
    "                             do_show_word_and_count_lists=False,\n",
    "                             axx=None\n",
    "                            ):\n",
    "    '''\n",
    "    @return  an axis from matplotlab (with the object - histogram - in it)\n",
    "    '''\n",
    "    \n",
    "    if axx is None:\n",
    "        fig = plt.figure(figsize=(10, 3))\n",
    "        axx = fig.add_subplot(111)\n",
    "    \n",
    "    counts_pre = list(word_count_ordered_dict.values())\n",
    "    words_pre  = list(word_count_ordered_dict.keys())\n",
    "    \n",
    "    counts = counts_pre[:n_top_words]\n",
    "    words  = words_pre[:n_top_words]\n",
    "    \n",
    "    ## making sure things were working\n",
    "    if do_show_word_and_count_lists:\n",
    "        print(f\"counts: {counts}\")\n",
    "        print(f\"words:  {words}\")\n",
    "    ##endof:  if do_show_word_and_count_lists\n",
    "    \n",
    "    x_words_coords = np.arange(len(words))\n",
    "    axx.bar(x_words_coords, counts, align='center')\n",
    "    \n",
    "    axx.set_xticks(x_words_coords)\n",
    "    axx.set_xticklabels(words, rotation=45, ha='right')\n",
    "    \n",
    "##endof:  get_histo_from_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histo_from_freq_dict(description_word_counts[desc_fname_idx_to_show], \n",
    "                         do_show_word_and_count_lists=True)\n",
    "\n",
    "desc_top_25_hist_fname = \"top_25_description_words.png\"\n",
    "plt.savefig(desc_top_25_hist_fname,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c005d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histo_from_freq_dict(application_word_counts[0],\n",
    "                         do_show_word_and_count_lists=True)\n",
    "\n",
    "appl_top_25_hist_fname = \"top_25_application_words.png\"\n",
    "plt.savefig(appl_top_25_hist_fname,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7ae60",
   "metadata": {},
   "source": [
    "## Output for Description and Application:\n",
    "\n",
    "### &lt;FILL THIS IN&gt;\n",
    "\n",
    "### Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c55c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1acab5",
   "metadata": {},
   "source": [
    "The output when I actually did this was\n",
    "\n",
    "```\n",
    "<Here is where the output will go>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c2fcd",
   "metadata": {},
   "source": [
    "### Change the img src values and img alt values, then see the histograms together\n",
    "\n",
    "You might need to double-click on the image to get the html source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5d651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  img src values for the two images:\")\n",
    "print(f'\"{desc_top_25_hist_fname}\"')\n",
    "print(f'\"{appl_top_25_hist_fname}\"')\n",
    "\n",
    "print()\n",
    "print(\"  img alt values for the two images:\")\n",
    "wd_count_alt_text_1 = '\"The histogram for the job description with word frequencies\"'\n",
    "wd_count_alt_text_2 = '\"The histogram for the job application with word frequencies\"'\n",
    "\n",
    "print(wd_count_alt_text_1)\n",
    "print(wd_count_alt_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d31453",
   "metadata": {},
   "source": [
    "The output histograms, stacked for easier view.\n",
    "\n",
    "_Remember that you might need to double click on the images to change the img src and img alt values._\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"top_25_description_words.png\"\n",
    "       alt=\"The histogram for the job description with word frequencies\"\n",
    "       width=\"auto\">\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"top_25_application_words.png\"\n",
    "       alt=\"The histogram for the job application with word frequencies\"\n",
    "       width=\"auto\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf6cde",
   "metadata": {},
   "source": [
    "Sometimes, I'll grab a printscreen of the above two images and draw green lines between words that match. However, from the time when I allowed the view of the match and three surrounding words, this step hasn't seemed as vital.\n",
    "\n",
    "If this is going to happen, double click on this cell to see the now-commented HTML, get your saved filename, change the HTML accordingly, and uncomment everything. (HTML Comments start with `<!--` and end with `-->`\n",
    "\n",
    "<!--\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"word_frequency_plots_w_link_lines.jpg\"\n",
    "       alt=\"Word matches for the pair of histograms.\"\n",
    "       width=\"100%\">\n",
    "</div>\n",
    "<br/>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e01967",
   "metadata": {},
   "source": [
    "## Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b13da",
   "metadata": {},
   "source": [
    "- Look at ranking, counts, percentage, etc. for FamilySearch's (job description's) top 25 words as found in my (job application's) word counts, then vice-versa. \n",
    "  - Code setup completed 2023-08-20. Putting all 25 in would make a very busy display, so I just did a few.\n",
    "- Get rid of words that are necessary for grammar, but which don't matter too much in determining whether the two documents match up. (Found term on 2023-08-07. It's \"stopwords\".)\n",
    "  - Completed 2023-08-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73f0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
