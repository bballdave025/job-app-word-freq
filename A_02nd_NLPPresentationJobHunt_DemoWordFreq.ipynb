{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d10fc3",
   "metadata": {},
   "source": [
    "# Job-Hunt NLP Demo\n",
    "\n",
    "Which demo will also be useful in doing some quick NLP work to see how my résumé's word distribution matches that from job descriptions.\n",
    "\n",
    "There's a wonderful project out there, [MyBinder](https://mybinder.org), which allows you to interactively run a Jupyter notebook completely online. It's nice to have when you'd like to play with code and see better the outputs that come from running that code. I've had some problems with images going down, but I'm going to work to keep this one up.\n",
    "\n",
    "The link to the online, interactive notebook - the binder - will be at the badge you see right here\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/bballdave025/ancestry-freq/main?labpath=Ancestry_NLP_Useful_and_Demo.ipynb)\n",
    "\n",
    "when I figure out how to get the Binder to stay.\n",
    "\n",
    "<strike>Go ahead and give it a try!</strike> \\[Soon!\\]\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f69f9",
   "metadata": {},
   "source": [
    "## Link to setup from the Conda Prompt\n",
    "\n",
    "The instructions for setting up the conda environment from Windows is in [my first version, here on GitHub](https://github.com/bballdave025/job-app-word-freq/blob/main/A_v01_NLP_Presentation_Job_Hunt_NLP_Useful_Demo_Word_Freq.ipynb). Soon, I will figure out how to make the [MyBinder](https://mybinder.org) server <strike>[MyBinder server]()</strike> for that first version persistent, and you can look at all the setup stuff there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d3fc8",
   "metadata": {},
   "source": [
    "## The Second Iteration\n",
    "\n",
    "### Several FamilySearch Résumés\n",
    "\n",
    "My friend at the [FamilySearch Library](https://www.familysearch.org/en/library/) let me know about a few job availabilities. These are all with a group - of which he and I are part - of missionaries and volunteers who have been working on [CJKV (Chinese, Japanese, Korean, Vietnamese)-character](https://en.wikipedia.org/wiki/CJKV_characters) handwriting and block-print recognition. I already put in the applications with résumés, but all résumés are pretty simple. I'm going to see how the different job descriptions compare to the résumés as regards the word-frequency distribution. \n",
    "\n",
    "I'm going to add some improvements to my first, time-limited version. These include the better-presentation output of the word and frequency arrays. I would also like to add something that removes small words that serve a more grammatical function; in the [NLTK book](https://www.nltk.org/book/) (Officially: BIRD, Steven; KLEIN, Ewan; and LOPER, Edward, Natural Language Processing with Python – Analyzing Text with the Natural Language Toolkit, retrieved 2023-08-07, https://web.archive.org/web/20230721043038/https://www.nltk.org/book/)\n",
    "Steven Bird, Ewan Klein, and Edward Loper), these are called stopwords (cf. Chapter 2). Perhaps I'll put in some n-gram comparison, but what I'd really like to do are space-separated bigrams - now I've looked it up, and the official term is #####."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2201e56",
   "metadata": {},
   "source": [
    "## Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d411d8",
   "metadata": {},
   "source": [
    "The text of my résumé for these jobs is in the local file,\n",
    "\n",
    "```\n",
    "res_CJKV.txt\n",
    "```\n",
    "\n",
    "the job descriptions for the jobs are in local files as well, specifically,\n",
    "\n",
    "```\n",
    "desc_CJKV_dev3.txt\n",
    "desc_CJKV_dev4.txt\n",
    "desc_CJKV_dev5.txt\n",
    "desc_CJKV_devInTest3.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b18797",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_text_filenames = \\\n",
    "  [\"res_CJKV.txt\",\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb355d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_text_filenames = \\\n",
    "  [\"desc_CJKV_dev5.txt\",\n",
    "   \"desc_CJKV_dev4.txt\",\n",
    "   \"desc_CJKV_dev3.txt\",\n",
    "   \"desc_CJKV_devInTest3.txt\",\n",
    "  ]\n",
    "\n",
    "# The \"dev5\" is the nicest job - and it's with Java, which I know best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca5e0c",
   "metadata": {},
   "source": [
    "`######################################################`\n",
    "\n",
    "The job description page looks to contain something like `JavaScript`, `ajax`, etc.\n",
    "\n",
    "Rather than writing in a webscraper or looking through the code and finding what gets pulled from the database, I'm just going to copy/paste the text into the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71eaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Code to get current timestamp, if needed.\n",
    "##+ Meant to be run once, then commented out.\n",
    "#######################\n",
    "# No need to run again\n",
    "#####\n",
    "!powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436591d",
   "metadata": {},
   "source": [
    "Output was\n",
    "\n",
    "```\n",
    "[The]\n",
    "[Lists]\n",
    "```\n",
    "\n",
    "at `Full-timestamp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c57519",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_job_desc_filenames = job_description_text_filenames\n",
    "local_job_app_filenames  = application_text_filenames\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(local_job_desc_filenames)\n",
    "pprint.pprint(local_job_app_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in texts to original strings\n",
    "\n",
    "complete_description_text = \" \"\n",
    "complete_application_text = \" \"\n",
    "\n",
    "for this_description_filename in local_job_desc_filenames:\n",
    "    with open(this_description_filename, 'r', encoding='utf-8') as dfh:\n",
    "        complete_description_text += dfh.read()\n",
    "    ##endof:  with open ... dfh\n",
    "##endof:  for this_description_filename in local_job_desc_filenames\n",
    "\n",
    "for this_application_filename in local_job_app_filenames:\n",
    "    with open(this_application_filename, 'r', encoding='utf-8') as afh:\n",
    "        complete_application_text += afh.read()\n",
    "    ##endof:  with open ... afh\n",
    "##endof:  for this_application_filename in local_job_app_filenames\n",
    "\n",
    "complete_description_text += \" \"\n",
    "complete_application_text += \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e6c18",
   "metadata": {},
   "source": [
    "### Code for cleaning text\n",
    "\n",
    "We will iterate a bit, so as not to have to write a text normalizer for the whole world. Rather than putting together regexes to test for things like which contractions are there and which other things might need changing (especially things like dashes), I'm doing simple regexes. Q&R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c83bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#from bs4 import UnicodeDammit\n",
    "\n",
    "def clean_text_string_quickly(input_str):\n",
    "    processing_str = input_str\n",
    "    \n",
    "    ## one line, single-spaced\n",
    "    processing_str = ' '.join(processing_str.split())\n",
    "    processing_str = processing_str.replace(\"\\t\", \" \")\n",
    "    processing_str = processing_str.replace(\"\\n\", \" \")\n",
    "    processing_str = re.sub(r\"([^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## get rid of outside-ascii (or control character)\n",
    "    processing_str = re.sub(r\"[^\\u0020-\\u007E]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## my stuff\n",
    "    processing_str = re.sub(r\"[ ][|]+[ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    processing_str = processing_str.replace(r\"&\", \"and\")\n",
    "    processing_str = processing_str.replace(r\"U.S.\", \"U S \")\n",
    "    \n",
    "    ## get rid of punctuation\n",
    "    processing_str = re.sub(r\"(([^0-9 ])[.,!?:\\\"']([) ]|$))\",\n",
    "                            r\"\\g<2>\\g<3>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    processing_str = re.sub(r\"(([0-9 ])[.,!?:\\\"']([ ]|$))\",\n",
    "                            r\"\\g<2>\\g<3>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    # parentheses\n",
    "    processing_str = processing_str.replace(r\"(\", \" \")\n",
    "    processing_str = processing_str.replace(r\")\", \" \")\n",
    "    # dashes\n",
    "    processing_str = re.sub(r\"[ ][-]+[ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ##  lowercase - to skip until a few iterations through\n",
    "    ##+ cleaning the text\n",
    "    processing_str = processing_str.casefold()\n",
    "    \n",
    "    ## fixes found by iterating this cleaning function\n",
    "    processing_str = re.sub(r\"[ ][/][ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    " \n",
    "    ##  From https://www.nltk.org/book/ch02.html\n",
    "    ##+ > [Stopwords are] high-frequency words like the, to and also that we \n",
    "    ##+ > sometimes want to filter out of a document before further processing. \n",
    "    ##+ > Stopwords usually have little lexical content, and their presence in \n",
    "    ##+ > a text fails to distinguish it from other texts.\n",
    "\n",
    "    stopwords_to_remove = [\n",
    "'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 'can', 'will', 'just', 'should', 'now'\n",
    "]\n",
    "    \n",
    "    # No attempt to optimize code, here. Q&R\n",
    "    for my_stopword in stopwords_to_remove:\n",
    "        processing_str = processing_str.replace(my_stopword, \" \")\n",
    "    ##endof:  for my_stopword in stopwords_to_remove:\n",
    "    \n",
    "    processing_str = processing_str.replace(r\"monitors/equipment\", \n",
    "                                                  \"monitors equipment\")\n",
    "    processing_str = processing_str.replace(r\"notice/more\", \n",
    "                                                  \"notice more\")\n",
    "    \n",
    "    ##spacing fix at the end\n",
    "    processing_str = re.sub(r\"([^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## Let's give it back\n",
    "    return processing_str\n",
    "\n",
    "##endof:  clean_text_string_quickly(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll usually keep these two.\n",
    "do_look_at_description_text = True\n",
    "do_look_at_application_text = True\n",
    "\n",
    "#  this one can go (False) if you don't want the big strings\n",
    "#+ i.e. you don't want the complete file contents\n",
    "do_print_the_big_strings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_description_text:\n",
    "    test1 = clean_text_string_quickly(complete_description_text)\n",
    "    if do_print_the_big_strings:\n",
    "        print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    test2 = clean_text_string_quickly(complete_application_text)\n",
    "    if do_print_the_big_strings:\n",
    "        print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at contractions\n",
    "if do_look_at_description_text:\n",
    "    print(re.findall(r\"\\b('[\\w']+\\b|[\\w']+'[\\w']+|[\\w']+')\\b\", test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    print(re.findall(r\"\\b('[\\w']+\\b|[\\w']+'[\\w']+|[\\w']+')\\b\", test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at all slashes\n",
    "if do_look_at_description_text:\n",
    "    print(re.findall(r\"\\b[\\w/]+/[\\w/]+\\b\", test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eaeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    print(re.findall(r\"\\b[\\w./]+/[\\w/]+\\b\", test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593ac2f",
   "metadata": {},
   "source": [
    "## Word Frequency Counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f8fb1",
   "metadata": {},
   "source": [
    "I want to use an `OrderedDict`, rather than mess with sorting the contents of a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_sorted_word_counts(*cleaned_strings):\n",
    "                           #,\n",
    "                           #do_output_sorted_file=False,\n",
    "                           #sorted_filename=\\\n",
    "                           #    \"sorted_words_from_strings.txt\"):\n",
    "    '''\n",
    "    @return  OrderedDict\n",
    "    '''\n",
    "    \n",
    "    EXIT_NOWORDSWEREFOUND = -1\n",
    "    \n",
    "    work_with_str = combine_strings(cleaned_strings)\n",
    "    \n",
    "    list_of_words_in_str = work_with_str.split()\n",
    "    \n",
    "    if len(list_of_words_in_str) <= 0:\n",
    "        print(\"No words were found.\", file=sys.stdout)\n",
    "        print(\"The program will exit.\", file=sys.stdout)\n",
    "        #sys.exit(EXIT_NOWORDSWEREFOUND)\n",
    "        return EXIT_NOWORDSWEREFOUND\n",
    "    ##endof:  if len(list_of_words_in_str) <= 0\n",
    "    \n",
    "    word_count_ordered_dict = OrderedDict()\n",
    "    \n",
    "    for this_word in list_of_words_in_str:\n",
    "        if this_word in word_count_ordered_dict:\n",
    "            word_count_ordered_dict[this_word] += 1\n",
    "        else:\n",
    "            word_count_ordered_dict[this_word] = 1\n",
    "        ##endof:  if/else this_word in list_of_words_in_str\n",
    "    ##endof:  for this_word in list_of_words_in_str\n",
    "    \n",
    "    ## DWB note ##\n",
    "    ##  At this point, the OrderedDict is sorted by the\n",
    "    ##+ order in which keys were inserted, not by their\n",
    "    ##+ count.\n",
    "    \n",
    "    for key, _ in \\\n",
    "          sorted(word_count_ordered_dict.items(),\n",
    "                 key=lambda word_and_count: word_and_count[1],\n",
    "                 reverse=True):\n",
    "        word_count_ordered_dict.move_to_end(key)\n",
    "    ##endof:  for myword, _ ...\n",
    "    \n",
    "    return word_count_ordered_dict\n",
    "    \n",
    "##endof:  get_sorted_word_counts(*cleaned_strings)\n",
    "\n",
    "def combine_strings(tuple_of_strings):\n",
    "                    #, \n",
    "                    #do_output_raw_file=False,\n",
    "                    #raw_filename='raw_words_from_strings.txt'):\n",
    "    '''\n",
    "    @return  string\n",
    "    '''\n",
    "    \n",
    "    returned_str = \" \"\n",
    "    \n",
    "    for this_str in tuple_of_strings:\n",
    "        returned_str += this_str + \" \"\n",
    "    ##endof:  for this_str in tuple_of_string\n",
    "    \n",
    "    ## one line, single-spaced\n",
    "    returned_str = ' '.join(returned_str.split())\n",
    "    returned_str = returned_str.replace(\"\\t\", \" \")\n",
    "    returned_str = returned_str.replace(\"\\n\", \" \")\n",
    "    returned_str = re.sub(r\"([^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            returned_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    return returned_str\n",
    "##endof:  combine_strings(tuple_of_strings)\n",
    "\n",
    "# @TODO: add a sort-by-word as well as sort-by-count flag\n",
    "# @TODO:  also, print out the pre-sorted and sorted files\n",
    "#       + with word lists, frequency, and in-order-of-\n",
    "#       + highest-count stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa30af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning and Counting ##\n",
    "\n",
    "description_str = clean_text_string_quickly(complete_description_text)\n",
    "application_str = clean_text_string_quickly(complete_application_text)\n",
    "\n",
    "description_word_counts = \\\n",
    "           get_sorted_word_counts(description_str)\n",
    "application_word_counts = \\\n",
    "           get_sorted_word_counts(application_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bcecc",
   "metadata": {},
   "source": [
    "The next code will take care of the issue I described thusly:\n",
    "> It's annoying me not to have a nice, aligned output for these 2d lists - basically, they're tables. I need to bring in some previous code that takes care of getting stuff printed nice. That will be after the Q&R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def print_2d_list_columns_aligned(list_2d_to_print,\n",
    "                                  joining_delimiter = \",  \",\n",
    "                                  do_output_as_str_and_print = False,\n",
    "                                  do_see_the_guts=False):\n",
    "    ##  The  do_see_the_guts boolean is partly for debugging,\n",
    "    ##+ partly for remembering and teaching how the process\n",
    "    ##+ works.\n",
    "    \n",
    "    ## Make all elements strings - so we can use len()\n",
    "    list_2d_all_strings = \\\n",
    "      [[str(item) for item in row] for row in list_2d_to_print]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_2d_all_strings:\")\n",
    "        print(list_2d_all_strings)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    #  We want to find the max string length for each column\n",
    "    #+ We can basically transpose the 2d_list to get the\n",
    "    #+ content of each column\n",
    "    list_of_column_elems_as_tuples = \\\n",
    "                 [column for column in zip(*list_2d_all_strings)]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_of_column_elems_as_tuples:\")\n",
    "        print(list_of_column_elems_as_tuples)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    ## find the max string length for each tuple (each column)\n",
    "    list_of_max_str_len_by_column = \\\n",
    "      [max([len(strng) for strng in tpl]) \n",
    "        for tpl in list_of_column_elems_as_tuples]\n",
    "   \n",
    "    # -v- Commented code\n",
    "    # -v- gives array with elements being each longest string\n",
    "    #[max([strng for strng in tpl], key=len) for tpl in list_of_column_elems_as_tuples]\n",
    "    # -v- 2d array with strings\n",
    "    #[[strng for strng in tpl] for tpl in list_of_column_elems_as_tuples]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_of_max_str_len_by_column:\")\n",
    "        print(list_of_max_str_len_by_column)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    # Create a formatter for each row\n",
    "    fmt_str = \\\n",
    "      joining_delimiter.join('{{:{}}}'.format(max_len) \n",
    "                               for max_len in list_of_max_str_len_by_column)\n",
    "    fmt_str = \"[\" + fmt_str + \"]\"\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  fmt_str:\")\n",
    "        print(fmt_str)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    # Get a string for each row, formatted correctly\n",
    "    list_of_formatted_row_strings = \\\n",
    "      [fmt_str.format(*row) for row in list_2d_all_strings]\n",
    "    \n",
    "    if do_see_the_guts:\n",
    "        print()\n",
    "        print(\"  list_of_formatted_row_strings:\")\n",
    "        print(list_of_formatted_row_strings)\n",
    "        print()\n",
    "    ##endof:  if do_see_the_guts\n",
    "    \n",
    "    if do_output_as_str_and_print:\n",
    "        return list_of_formatted_row_strings\n",
    "    ##endof:  if do_output_as_str_and_print\n",
    "    \n",
    "##endof:  print_2d_list_colunns_aligned(<params>)\n",
    "\n",
    "#print_2d_list_columns_aligned(matrix, do_see_the_guts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b66e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Only change this boolean if you want to see a lot of output. ##\n",
    "do_print_long_full_version = False\n",
    "\n",
    "#import pprint\n",
    "\n",
    "if do_print_long_full_version:\n",
    "    dashes=\"------------------------------------------------------------\"\n",
    "    short_dashes=\"-----\"\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(dashes)\n",
    "    print(\" For the job description:\")\n",
    "    print(short_dashes)\n",
    "    #pprint.pprint(description_word_counts)\n",
    "    print_2d_list_columns_aligned(description_word_counts)\n",
    "    print(dashes)\n",
    "    print()\n",
    "    print()\n",
    "    print(dashes)\n",
    "    print(\" For the job application materials (résumé, cover letter, etc.):\")\n",
    "    print(short_dashes)\n",
    "    #pprint.pprint(application_word_counts)\n",
    "    print_2d_list_columns_aligned(application_word_counts)\n",
    "    print(dashes)\n",
    "    print()\n",
    "    print()\n",
    "##endof:  if do_print_long_full_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_simpler = False\n",
    "\n",
    "##  We will do\n",
    "table_version_desc = [[\"desc_word\", \"desc_cnt\", \"desc_rank\"],]\n",
    "table_version_appl = [[\"appl_word\", \"appl_cnt\", \"appl_rank\"],]\n",
    "table_version_both = [[\"rank\", \"desc_word\", \"desc_cnt\", \n",
    "                       \"appl_word\", \"appl_cnt\"],\n",
    "                     ]\n",
    "\n",
    "description_items = list(description_word_counts.items())\n",
    "application_items = list(application_word_counts.items())\n",
    "\n",
    "n_words_description = len(description_items)\n",
    "n_words_application = len(application_items)\n",
    "\n",
    "print()\n",
    "print(\"NOTE THAT THESE ARE THE NUMBERS OF DISTINCT WORDS\")\n",
    "print(f\"n_words_description = {str(n_words_description)}\")\n",
    "print(f\"n_words_application = {str(n_words_application)}\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "if try_simpler:\n",
    "    import pprint\n",
    "    print()\n",
    "    print()\n",
    "    print(\"DESCRIPTION\")\n",
    "    pprint.pprint(description_items)\n",
    "    print()\n",
    "    print(f\"n_words_description = {str(n_words_description)}\")\n",
    "    print()\n",
    "    print()\n",
    "    print(\"APPLICATION\")\n",
    "    pprint.pprint(application_items)\n",
    "    print()\n",
    "    print(f\"n_words_application = {str(n_words_application)}\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"NOTE THAT THESE ARE THE NUMBERS OF DISTINCT WORDS\")\n",
    "    print(f\"n_words_description = {str(n_words_description)}\")\n",
    "    print(f\"n_words_application = {str(n_words_application)}\")\n",
    "    print()\n",
    "##endof:  if try_simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for this_idx in range(max(len(description_items),\n",
    "                          len(application_items)\n",
    "                         )\n",
    "                      ):\n",
    "    this_rank = this_idx + 1\n",
    "    \n",
    "    if this_idx < len(description_items) - 1:\n",
    "        try:\n",
    "            this_description_word  = description_items[this_idx+1][0]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR desc word!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(description_items): {str(len(description_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(description_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        try:\n",
    "            this_description_count = description_items[this_idx+1][1]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR desc count!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(description_items): {str(len(description_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(description_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        this_description_rank  = this_rank\n",
    "    else:\n",
    "        this_description_word  = \"-- N/A --\"\n",
    "        this_description_count = \"-- N/A --\"\n",
    "        this_description_rank  = \"-- N/A --\"\n",
    "    ##endof:  if/else this_idx < len(description_items)\n",
    "    \n",
    "    if this_idx < len(application_items) - 1:\n",
    "        try:\n",
    "            this_application_word  = application_items[this_idx+1][0]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR appl word!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(application_items): {str(len(application_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(application_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        try:\n",
    "            this_application_count = application_items[this_idx+1][1]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR appl count!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(application_items): {str(len(application_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(application_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        this_application_rank  = this_rank\n",
    "    else:\n",
    "        this_application_word  = \"-- N/A --\"\n",
    "        this_application_count = \"-- N/A --\"\n",
    "        this_application_rank  = \"-- N/A --\"\n",
    "    ##endof:  if/else this_idx < len(application_items)\n",
    "    \n",
    "    if this_description_word != \"-- N/A --\":\n",
    "        try:\n",
    "            table_version_desc.append([this_description_word, \n",
    "                                       this_description_count, \n",
    "                                       this_rank]\n",
    "                                     )\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR desc!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"this_description_word: {str(this_description_word)}\", file=sys.stdout)\n",
    "            print(f\"this_description_count: {str(this_description_count)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/except/finally\n",
    "        \n",
    "    ##endof:  if this_description_word != \"-- N/A --\"\n",
    "    \n",
    "    if this_application_word != \"-- N/A --\":\n",
    "        try:\n",
    "            table_version_appl.append([this_application_word, \n",
    "                                       this_application_count, \n",
    "                                       this_rank\n",
    "                                      ]\n",
    "                                     )\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR appl!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {this_idx}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"this_application_word: {str(this_application_word)}\", file=sys.stdout)\n",
    "            print(f\"this_application_count: {str(this_application_count)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/except/finally\n",
    "        \n",
    "    ##endof:  if this_application_word != \"-- N/A --\"\n",
    "    \n",
    "    try:\n",
    "        table_version_both.append([this_rank, \n",
    "                                   this_description_word, \n",
    "                                   this_description_count,\n",
    "                                   this_application_word, \n",
    "                                   this_application_count\n",
    "                                  ]\n",
    "                                 )\n",
    "    except IndexError as ie:\n",
    "        print(\"ERROR both!\", file=sys.stdout)\n",
    "        print(str(ie), file=sys.stdout)\n",
    "        print(f\"this_idx: {this_idx}\", file=sys.stdout)\n",
    "        print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "        print(f\"this_description_word: {str(this_description_word)}\", file=sys.stdout)\n",
    "        print(f\"this_description_count: {str(this_description_count)}\", file=sys.stdout)\n",
    "        print(f\"this_application_word: {str(this_application_word)}\", file=sys.stdout)\n",
    "        print(f\"this_application_count: {str(this_application_count)}\", file=sys.stdout)\n",
    "    finally:\n",
    "        pass\n",
    "    ##endof:  try/except/finally\n",
    "        \n",
    "##endof:  for this_idx in .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "##  Set up the display the first n_lines_to_display \n",
    "##+ of the tables, nicely\n",
    "\n",
    "#### This next one is the one you might change\n",
    "n_lines_to_display_orig = 25\n",
    "\n",
    "n_header_lines = 1\n",
    "n_lines_to_display = n_lines_to_display_orig\n",
    "n_lines_to_display_desc = n_lines_to_display_orig\n",
    "n_lines_to_display_appl = n_lines_to_display_orig\n",
    "\n",
    "do_cut_down_desc = ( n_lines_to_display_orig >\n",
    "                             len(table_version_desc) \n",
    "                   )\n",
    "if do_cut_down_desc:\n",
    "    n_lines_to_display_desc = len(table_version_desc)\n",
    "##endof:  if do_cut_down_desc\n",
    "\n",
    "do_cut_down_appl = ( n_lines_to_display_orig >\n",
    "                             len(table_version_appl) \n",
    "                   )\n",
    "if do_cut_down_appl:\n",
    "    n_lines_to_display_appl = len(table_version_appl)\n",
    "##endof:  if do_cut_down_appl\n",
    "\n",
    "# get headers\n",
    "display_table_desc = [table_version_desc[0]]\n",
    "display_table_appl = [table_version_appl[0]]\n",
    "display_table_both = [table_version_both[0]]\n",
    "\n",
    "if ( len(table_version_desc) - n_header_lines < n_lines_to_display or\n",
    "     len(table_version_appl) - n_header_lines < n_lines_to_display\n",
    "   ):\n",
    "    n_lines_to_display = min(len(table_version_desc) - n_header_lines,\n",
    "                             len(table_version_appl) - n_header_lines)\n",
    "##endof:  if <n_lines_conditions>\n",
    "\n",
    "\n",
    "for table_idx in range(n_header_lines, \n",
    "                       n_lines_to_display_orig + n_header_lines):\n",
    "    if table_idx - n_header_lines < n_lines_to_display_desc:\n",
    "        try:\n",
    "            display_table_desc.append(table_version_desc[table_idx])\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR display_table_desc!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_desc: {n_lines_to_display_desc}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_desc): {len(table_version_desc)}\", file=sys.stdout)\n",
    "        except Error as e:\n",
    "            print(\"DIFFERENT ERROR display_table_desc!\", file=sys.stdout)\n",
    "            print(str(e), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_desc: {n_lines_to_display_desc}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_desc): {len(table_version_desc)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "    ##endof:  if table_idx - n_header_lines < n_lines_to_display_desc\n",
    "    \n",
    "    if table_idx - n_header_lines < n_lines_to_display_appl:\n",
    "        try:\n",
    "            display_table_appl.append(table_version_appl[table_idx])\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR display_table_appl!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_appl: {n_lines_to_display_appl}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_appl): {len(table_version_appl)}\", file=sys.stdout)\n",
    "        except Error as e:\n",
    "            print(\"DIFFERENT ERROR display_table_appl!\", file=sys.stdout)\n",
    "            print(str(e), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_appl: {n_lines_to_display_appl}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_appl): {len(table_version_appl)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "    ##endof:  if table_idx - n_header_lines < n_lines_to_display_appl\n",
    "    \n",
    "    try:\n",
    "        display_table_both.append(table_version_both[table_idx])\n",
    "    except IndexError as ie:\n",
    "        print(\"ERROR display_table_both!\", file=sys.stdout)\n",
    "        print(str(ie), file=sys.stdout)\n",
    "        print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "        print(f\"len(table_version_both): {len(table_version_both)}\", file=sys.stdout)\n",
    "    finally:\n",
    "        pass\n",
    "    ##endof:  try/catch/finally\n",
    "##endof:  for idx in range(<n_lines stuff>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(table_version_desc): {len(table_version_desc)}\")\n",
    "print(f\"len(display_table_desc): {len(display_table_desc)}\")\n",
    "print()\n",
    "print(f\"len(table_version_appl): {len(table_version_appl)}\")\n",
    "print(f\"len(display_table_appl): {len(display_table_appl)}\")\n",
    "print()\n",
    "print(f\"len(table_version_both): {len(table_version_both)}\")\n",
    "print(f\"len(display_table_both): {len(display_table_both)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pprint\n",
    "\n",
    "long_dashes = \"-----------------------------------------------\"\n",
    "short_dashes = \"-----\"\n",
    "\n",
    "print()\n",
    "print(long_dashes)\n",
    "print(\"JOB DESCRIPTION (TOP 25)\")\n",
    "print(short_dashes)\n",
    "#pprint.pprint(display_table_desc)\n",
    "print_2d_list_columns_aligned(display_table_desc)\n",
    "print()\n",
    "print(long_dashes)\n",
    "print()\n",
    "print()\n",
    "print(long_dashes)\n",
    "print(\"JOB APPLICATION STUFF - RéSUMé AND COVER LETTER (TOP 25)\")\n",
    "print(short_dashes)\n",
    "#pprint.pprint(display_table_appl)\n",
    "print_2d_list_columns_aligned(display_table_appl)\n",
    "print()\n",
    "print(long_dashes)\n",
    "print()\n",
    "print()\n",
    "print(long_dashes)\n",
    "print(\"COMPARISON OF DESCRIPTION AND APPLICATION (TOP 25)\")\n",
    "print(short_dashes)\n",
    "#pprint.pprint(display_table_both)\n",
    "print_2d_list_columns_aligned(display_table_both)\n",
    "print()\n",
    "print(long_dashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482669e",
   "metadata": {},
   "source": [
    "## Time for histograms (or whatever the discretized version is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5092550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Next line only for Jupyter notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def get_histo_from_freq_dict(word_count_ordered_dict,\n",
    "                             n_top_words = 25,\n",
    "                             do_show_word_and_count_lists=False,\n",
    "                             axx=None\n",
    "                            ):\n",
    "    '''\n",
    "    @return  an axis from matplotlab (with the object - histogram - in it)\n",
    "    '''\n",
    "    \n",
    "    if axx is None:\n",
    "        fig = plt.figure(figsize=(10, 3))\n",
    "        axx = fig.add_subplot(111)\n",
    "    \n",
    "    counts_pre = list(word_count_ordered_dict.values())\n",
    "    words_pre  = list(word_count_ordered_dict.keys())\n",
    "    \n",
    "    counts = counts_pre[:n_top_words]\n",
    "    words  = words_pre[:n_top_words]\n",
    "    \n",
    "    ## making sure things were working\n",
    "    if do_show_word_and_count_lists:\n",
    "        print(f\"counts: {counts}\")\n",
    "        print(f\"words:  {words}\")\n",
    "    ##endof:  if do_show_word_and_count_lists\n",
    "    \n",
    "    x_words_coords = np.arange(len(words))\n",
    "    axx.bar(x_words_coords, counts, align='center')\n",
    "    \n",
    "    axx.set_xticks(x_words_coords)\n",
    "    axx.set_xticklabels(words, rotation=45, ha='right')\n",
    "    \n",
    "##endof:  get_histo_from_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histo_from_freq_dict(description_word_counts, do_show_word_and_count_lists=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c005d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histo_from_freq_dict(application_word_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7ae60",
   "metadata": {},
   "source": [
    "## Output for Description and Application:\n",
    "\n",
    "### &lt;FILL THIS IN&gt;\n",
    "\n",
    "### Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c55c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1acab5",
   "metadata": {},
   "source": [
    "The output when I actually did this was\n",
    "\n",
    "```\n",
    "<Here is where the output will go>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d31453",
   "metadata": {},
   "source": [
    "The output histograms, in an image.\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"first_QandR_word_frequency_plots.jpg\"\n",
    "       alt=\"The first pair of histograms - one for the job description, one for the job application - with word frequencies\"\n",
    "       width=\"100%\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf6cde",
   "metadata": {},
   "source": [
    "Here is some idea of how they match. I hope it makes some sense. Darker green means an exact match; thinner dark green means a match with words that don't add much meaning; lighter green means it's a close match. \n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"first_QandR_word_frequency_plots_w_link_lines.jpg\"\n",
    "       alt=\"Word matches for the first pair of histograms.\"\n",
    "       width=\"100%\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e01967",
   "metadata": {},
   "source": [
    "## Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b13da",
   "metadata": {},
   "source": [
    "- Look at ranking, counts, percentage, etc. for FamilySearch's (job description's) top 25 words as found in my (job application's) word counts, then vice-versa. \n",
    "- Get rid of words that are necessary for grammar, but which don't matter too much in determining whether the two documents match up. (Found term on 2023-08-17. It's \"stopwords\".)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73f0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
