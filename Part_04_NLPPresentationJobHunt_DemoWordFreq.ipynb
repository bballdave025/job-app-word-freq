{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d10fc3",
   "metadata": {},
   "source": [
    "# Job-Hunt NLP Demo - Part 4\n",
    "\n",
    "Which demo will also be useful in doing some quick NLP work to see how my résumé's word distribution matches that from job descriptions.\n",
    "\n",
    "There's a wonderful project out there, [MyBinder](https://mybinder.org), which allows you to interactively run a Jupyter notebook completely online. It's nice to have when you'd like to play with code and see better the outputs that come from running that code. I've had some problems with images going down, but I'm going to work to keep this one up.\n",
    "\n",
    "The link to the online, interactive notebook - the binder - is at the badge you see right here\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/bballdave025/job-app-word-freq/main?labpath=Part_04_NLPPresentationJobHunt_DemoWordFreq.ipynb)\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## We are calling this version 0.1.003\n",
    "\n",
    "It's the FamilySearch CJKV jobs applied for in August 2023, but we're splitting it into smaller notebooks. Hopefully, MyBinder can load each more quickly. We'll see how things work with pickling variables between the parts.\n",
    "\n",
    "**Edit:** It worked pretty well.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "## What we are doing in Part 4\n",
    "\n",
    "@TODO\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd5a12",
   "metadata": {},
   "source": [
    "## Let's start by un-pickle-ing the things we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae198da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_filename_3_to_4 = \"important_part_3_vars.pkl\"\n",
    "\n",
    "unpickled_array = []\n",
    "\n",
    "with open(pickle_filename_3_to_4, 'rb') as pfh:\n",
    "    unpickled_array = pickle.load(pfh)\n",
    "##endof:  with open ... as pfh # (pickle file handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b80b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_job_desc_filenames = unpickled_array[0]\n",
    "local_job_appl_filenames = unpickled_array[1]\n",
    "description_word_counts = unpickled_array[2]\n",
    "application_word_counts = unpickled_array[3]\n",
    "list_of_display_table_desc = unpickled_array[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41410d4",
   "metadata": {},
   "source": [
    "## Comparing a specific word in both distributions - nice visual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b865afb",
   "metadata": {},
   "source": [
    "### First, a review of where we've been @TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac3a13",
   "metadata": {},
   "source": [
    "@TODO Review what we've done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff19c1b",
   "metadata": {},
   "source": [
    "Good things are what we've done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e373b5d",
   "metadata": {},
   "source": [
    "From our previous stuff, there are a couple of nice functions that we're just copying and pasting, because it seems transferring functions via pickle file is a nightmare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d2ec6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description_word_at_rank(this_rank = 1, \n",
    "                                 this_desc_fname_idx=0\n",
    "                                ):\n",
    "    this_idx = this_rank # the header is index 0\n",
    "    this_table_to_use = \\\n",
    "      list_of_display_table_desc[this_desc_fname_idx]\n",
    "    this_word = this_table_to_use[this_rank][0]\n",
    "        \n",
    "    return this_word\n",
    "##endof:  get_description_word_at_rank(<params>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61bd2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_application_word_at_rank(this_rank = 1):\n",
    "    this_idx = this_rank # the header is index 0\n",
    "    this_appl_fname_idx=0\n",
    "    this_table_to_use = \\\n",
    "      list_of_display_table_appl[this_appl_fname_idx]\n",
    "    this_word = this_table_to_use[this_rank][0]\n",
    "    \n",
    "    return this_word\n",
    "##endof:  get_description_word_at_rank(<params>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ab4fd2",
   "metadata": {},
   "source": [
    "## A better way to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a832f5e7",
   "metadata": {},
   "source": [
    "### Seems like a good time to look at comparisons\n",
    "\n",
    "#### Between the résumé and the different job descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300922dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_word_in_both_display_lists(word_to_find,\n",
    "                                    display_list_1_description,\n",
    "                                    display_list_2_application,\n",
    "                                    name_of_display_list_1=None,\n",
    "                                    name_of_display_list_2=None,\n",
    "                                    do_print_details=False\n",
    "                                   ):\n",
    "    index_count_1 = 0 # skip header\n",
    "    index_for_found_in_1 = 0\n",
    "    \n",
    "    #loop_display_list = display_list_1_description\n",
    "    \n",
    "    word_found_in_1 = False\n",
    "    for my_entry_1 in display_list_1_description:\n",
    "        index_count_1 += 1\n",
    "        if my_entry_1 == word_to_find:\n",
    "            word_found_in_1 = True\n",
    "            index_for_found_in_1 = index_count_1\n",
    "            break\n",
    "        ##endof:  if my_entry_1 == word_to_find\n",
    "    ##endof:  for my_entry_1 in display_list_1\n",
    "    \n",
    "    index_count_2 = 0 # skip header\n",
    "    index_for_found_in_2 = -1\n",
    "    word_found_in_2 = False\n",
    "    for my_entry_2 in display_list_2_application:\n",
    "        index_count_2 += 1\n",
    "        if my_entry_2 == word_to_find:\n",
    "            word_found_in_2 = True\n",
    "            index_for_found_in_2 = index_count_2\n",
    "            break\n",
    "        ##endof:  if my_entry_2 == word_to_find\n",
    "    ##endof:  for my_entry_1 in display_list_1\n",
    "    \n",
    "    to_return_found_1 = None\n",
    "    \n",
    "    if word_found_in_1:\n",
    "        to_return_found_1 = index_for_found_in_1 # - 1\n",
    "        if do_print_details:\n",
    "            print()\n",
    "            print(f\"The word, {word_to_find}, has rank, {to_return_found_1},\")\n",
    "            if name_of_display_list_1 is not None:\n",
    "                print(f\"in list, {name_of_display_list_1}.\")\n",
    "            #endof:  if name_of_display_list_1 is not None\n",
    "        ##endof:  if do_print_details\n",
    "    ##endof:  if word_found_in_1\n",
    "    \n",
    "    to_return_found_2 = None\n",
    "    \n",
    "    if word_found_in_2:\n",
    "        to_return_found_2 = index_for_found_in_2 # - 1\n",
    "        if do_print_details:\n",
    "            print()\n",
    "            print(f\"The word, {word_to_find}, has rank, {to_return_found_2},\")\n",
    "            if name_of_display_list_2 is not None:\n",
    "                print(f\"in list, {name_of_display_list_2}.\")\n",
    "            ##endof:  if name_of_display_list_2 is not None\n",
    "        ##endof:  if do_print_details\n",
    "    ##endof:  if word_found_in_2\n",
    "    \n",
    "    return to_return_found_1, to_return_found_2\n",
    "    \n",
    "##endof:  find_word_in_both_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2590f305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Next line only for Jupyter notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def get_freq_histo_specific(word_count_ordered_dict_1,\n",
    "#                           #word_count_ordered_dict_2,\n",
    "                            rank_index_1 = 1,\n",
    "                            n_surrounding_words = 3,\n",
    "                            do_show_word_and_count_lists=False,\n",
    "                            do_show_frac_not_count=True,\n",
    "                            ylim_bottom_val=None,\n",
    "                            ylim_top_val=None,\n",
    "                            ax1=None,\n",
    "                            #ax2=None,\n",
    "                           ):\n",
    "    '''\n",
    "    @return  an axis from matplotlab (with the object - histogram - in it)\n",
    "    '''\n",
    "    \n",
    "    if ax1 is None:\n",
    "        fig = plt.figure(figsize=(10, 3))\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        #ax2 = fig.add_subplot(121)\n",
    "    ##endof:  if ax1 is None\n",
    "    \n",
    "    counts_pre = None\n",
    "    fractions_pre = None\n",
    "    \n",
    "    if do_show_frac_not_count:\n",
    "        frac_wd_cnt_list_of_tuples = \\\n",
    "          [ ( k, (v / len(word_count_ordered_dict_1)) )\n",
    "                  for k, v in word_count_ordered_dict_1.items() ]\n",
    "        fractions_pre = [ this_item[1]\n",
    "                           for this_item in frac_wd_cnt_list_of_tuples ]\n",
    "    else:\n",
    "        counts_pre = list(word_count_ordered_dict_1.values())\n",
    "    ##endof:  if/else do_show_frac_not_count\n",
    "    \n",
    "    words_pre  = list(word_count_ordered_dict_1.keys())\n",
    "    \n",
    "    counts = None\n",
    "    fractions = None\n",
    "    \n",
    "    # Pad the list with zero-count and empty-set characters\n",
    "    len_lists = 2 * n_surrounding_words + 1\n",
    "    \n",
    "    if do_show_frac_not_count:\n",
    "        fractions = [0] * len_lists\n",
    "    else:\n",
    "        counts = [0] * len_lists\n",
    "    ##endof:  if/else do_show_frac_not_count\n",
    "    \n",
    "    words  = [\"\\u2205\"] * len_lists # empty-set glyph codepoint\n",
    "    \n",
    "    #  Fill anything with a valid index with the corresponding\n",
    "    #+ word/count\n",
    "    \n",
    "    current_output_index = -1\n",
    "    \n",
    "    for i in range(rank_index_1 - n_surrounding_words -1,\n",
    "                   rank_index_1 + n_surrounding_words\n",
    "                  ):\n",
    "        current_output_index += 1\n",
    "        if i < 0:\n",
    "            pass\n",
    "        else:\n",
    "            if do_show_frac_not_count:\n",
    "                fractions[current_output_index] = fractions_pre[i]\n",
    "            else:\n",
    "                counts[current_output_index] = counts_pre[i]\n",
    "            ##endof:  if/else do_show_frac_not_count\n",
    "            \n",
    "            words[current_output_index] = words_pre[i]\n",
    "        ##endof:  if/else i < 1\n",
    "    ##endof:  for i in range\n",
    "    \n",
    "    ## making sure things are working\n",
    "    if do_show_word_and_count_lists:\n",
    "        if do_show_frac_not_count:\n",
    "            print(f\"fractions: {fractions}\")\n",
    "        else:\n",
    "            print(f\"counts: {counts}\")\n",
    "        ##endof:  if/else do_show_frac_not_count\n",
    "        \n",
    "        print(f\"words:  {words}\")\n",
    "    ##endof:  if do_show_word_and_count_lists\n",
    "    \n",
    "    x_words_coords = np.arange(len(words))\n",
    "    \n",
    "    if do_show_frac_not_count:\n",
    "        ax1.bar(x_words_coords, fractions, align='center')\n",
    "    else:\n",
    "        ax1.bar(x_words_coords, counts, align='center')\n",
    "    ##endof:  if/else do_show_frac_not_count\n",
    "    \n",
    "    ax1.set_xticks(x_words_coords)\n",
    "    ax1.set_xticklabels(words, rotation=45, ha='right')\n",
    "    \n",
    "    ax1.set_ylim(ylim_bottom_val, ylim_top_val)\n",
    "    \n",
    "##endof:  get_freq_histo_specific"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fe1780",
   "metadata": {},
   "source": [
    "<strike>Below will be code to look for the top 25 (maybe less, maybe more) description words. I'll go through every word that appears 3 times, and I won't include any that appear only twice or once. I'll see where they appear in my résumé list.</strike>\n",
    "\n",
    "<strike>This will be easily automated and done with a for loop or list comprehension. However, I want to look at some things more manually - that should make the automated stuff better.</strike>\n",
    "\n",
    "I'm going to make this part more of a look-for-each-word thing. The display is too busy to show each word for each file.\n",
    "\n",
    "I have a few improvements that would be good, soon:<br/>\n",
    "  @TODO : get rid of one letter words<br/>\n",
    "  @TODO : look through the rest of the list to get rid of junk\n",
    "\n",
    "I want to match two histograms for this stuff, with e.g. the job description's word and (up to) 3 (or 4 or 5 or 6 or 2 or 1 or ...) words more frequent and (up to) 3 words less frequent. I'm going to bring up a picture of the histograms for my brainstorming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d1403",
   "metadata": {},
   "source": [
    "### Here are the specific word-rank comparison histograms..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdc3071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_for_choices = f\"Choices are any of: {list(range(len(local_job_desc_filenames)))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04621fa6",
   "metadata": {},
   "source": [
    "####  For this section, we have calculated everything, but show just two files being compared\n",
    "\n",
    "Well, when we get to the compare-all-top-25 histograms, we'll show all the comparisons.\n",
    "\n",
    "For the comparisons of the top-ranked words, just two files at a time\n",
    "\n",
    "Another thing, to keep this Quick and Reckless (not spending too much time), I'm dispensing with my cherished 80 characters per line. `: (`\n",
    "\n",
    "<b>You can change the `desc_fname_idx_to_show` to any of the numbers in the next output ...</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "337c1a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choices are any of: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "print(str_for_choices.replace(r\"[\", r\"{\").replace(r\"]\", r\"}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80631c39",
   "metadata": {},
   "source": [
    "Output was most recently\n",
    "\n",
    "Choices are any of: `{0, 1, 2, 3}`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4573c2",
   "metadata": {},
   "source": [
    "<b>... to see results for a specific job description.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab896776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice 0 : desc_CJKV_dev5.txt\n",
      "Choice 1 : desc_CJKV_dev4.txt\n",
      "Choice 2 : desc_CJKV_dev3.txt\n",
      "Choice 3 : desc_CJKV_devInTest3.txt\n"
     ]
    }
   ],
   "source": [
    "for my_desc_index in range(len(local_job_desc_filenames)):\n",
    "    print(f\"Choice {my_desc_index} : {local_job_desc_filenames[my_desc_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba3985d",
   "metadata": {},
   "source": [
    "### For your choice of job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24993050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make your choice:\n",
    "desc_fname_idx_to_show = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f7f2d1",
   "metadata": {},
   "source": [
    "### Now we can continue with the top-ranked word in the job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f472c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(rank in description, rank in application) for the word, 'software': (1, 2)\n"
     ]
    }
   ],
   "source": [
    "top_word_rank_in_desc = 1\n",
    "this_corresponding_word = get_description_word_at_rank(top_word_rank_in_desc)\n",
    "this_desc_idx = desc_fname_idx_to_show\n",
    "\n",
    "word_indexes = find_word_in_both_display_lists(\n",
    "                 this_corresponding_word,\n",
    "                 description_word_counts[desc_fname_idx_to_show],\n",
    "                 application_word_counts[0],\n",
    "                   #  we only have one table - \n",
    "                   #+ it's at any legal index;\n",
    "                   #+ let's choose 0\n",
    "                 name_of_display_list_1 = f\"description_word_counts[{this_desc_idx}]\",\n",
    "                 name_of_display_list_2 = \"application_word_counts[0]\"                \n",
    ")\n",
    "\n",
    "print()\n",
    "print( (\"(rank in description, rank in application) for the word,\"\n",
    "        f\" '{this_corresponding_word}': {word_indexes}\"\n",
    "       )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc15ec5",
   "metadata": {},
   "source": [
    "### You might need to look back at the frequency-as-fraction histogram\n",
    "\n",
    "So you can set an appropriate maximum frequency to plot.\n",
    "\n",
    "#### Until I run through the math of Zipf's law with Mandlebrot's generalization\n",
    "\n",
    "Sources:\n",
    " - A discussion [here](https://web.archive.org/web/20230825202630/https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/)\n",
    " - Original Papers (Note that these are Google Scholar entries for books or papers. Oh, and they're the archived versions from the archive.org Wayback Machine.)\n",
    "   - Zipf [1936](https://web.archive.org/web/20230825203006/https://scholar.google.com/scholar_lookup?title=The+Psychobiology+of+Language&author=G+Zipf&publication_year=1936&), [1949](https://web.archive.org/web/20230825203504/https://scholar.google.com/scholar_lookup?title=Human+Behavior+and+the+Principle+of+Least+Effort&author=G+Zipf&publication_year=1949&)\n",
    "   - Mandelbrot [1953](https://web.archive.org/web/20230825204141/https://scholar.google.com/scholar_lookup?journal=Communication+theory&title=An+informational+theory+of+the+statistical+structure+of+language&author=B+Mandelbrot&publication_year=1953&pages=486-502&) (also available as [PDF](https://web.archive.org/web/20230825204201/http://pdodds.w3.uvm.edu/research/papers/others/1953/mandelbrot1953a.pdf)), [1962](https://web.archive.org/web/20230825204400/https://scholar.google.com/scholar_lookup?journal=Structure+of+language+and+its+mathematical+aspects&title=On+the+theory+of+word+frequencies+and+on+related+markovian+models+of+discourse&author=B+Mandelbrot&publication_year=1962&pages=190-219&) (also available as [PDF](https://web.archive.org/web/20230825204650/https://users.math.yale.edu/~bbm3/web_pdfs/024jakobson.pdf))\n",
    "   \n",
    "`The rth most frequent word has a frequecy` $f(r)$ `that scales according to`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566577a7",
   "metadata": {},
   "source": [
    "$f(r) \\propto 1 / \\left[(r + \\beta)^\\alpha\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431cf1e",
   "metadata": {},
   "source": [
    "`with` $\\alpha \\approx 1$ `and` $\\beta \\approx 2.7$. If we take the frequency of the word at rank $r$ as a fraction of the total words, and considering a text with $N$ distinct words, then the fractional frequency of a word with rank, $r$, will be $f(r) / N$. The sum of these fractions is one. So basically, we have $K \\cdot \\sum_{r=1}^N { f(r) / N} = 1$, with $K$ being the constant of proportionality. To make things a little easier as we go forward, I'll also define $\\kappa \\stackrel{\\text{def}}{=} 1 / K$\n",
    "\n",
    "\n",
    "\n",
    "`  ` $\\Rightarrow$ `  ` $\\kappa$ `= ` $\\frac{1}{N} \\cdot f(r)$\n",
    "\n",
    "`            = (1/N) * [1/(1+2.7) + 1/(2+2.7) + 1/(3+2.7) + ... + 1/{(N-1)+2.7} + 1/(N+2.7)]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b36e31",
   "metadata": {},
   "source": [
    "So, all I need to do is consider a text with $N$ distinct words and calculate $\\kappa$, then the frequency of appearance for the word having rank, $r$, as a fraction of total words would be $\\left(K / N \\right) \\cdot \\left[1 / \\left(r+2.7\\right)\\right]$. \n",
    "\n",
    "Let's designate $\\pi(r)$ be the proportion of total words that the word of rank $r$ makes of the total - the same thing we've been designating as the frequency of appearance as a fraction.\n",
    "\n",
    "This then gives us\n",
    "\n",
    "$\\pi(r) \\approx \\left[1 / (N \\kappa)\\right] \\cdot f(r) = \\left( \\frac{1}{N \\cdot \\kappa} \\right) \\cdot \\sum_{r=1}^N \\left[ 1 / \\left( r + \\beta \\right) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366d346f",
   "metadata": {},
   "source": [
    "Let's define $P$ as our partial sum, i.e.\n",
    "\n",
    "$P \\stackrel{\\text{def}}{=} \\sum_{r=1}^N \\left[1 / \\left(r + \\beta\\right)\\right]$\n",
    "\n",
    "We then obtain\n",
    "\n",
    "$\\pi(r) \\approx P / \\left( N \\cdot \\kappa \\right)$\n",
    "\n",
    "Once again, we have $\\pi(r)$ defined as the frequency as a fraction. (It is the proportion of all words which are the word with rank, $r$. One could also think of it as the probability of the word.)\n",
    "\n",
    "Remember in all this that $\\beta \\approx 2.7$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d910ca",
   "metadata": {},
   "source": [
    "### Looks like time for partial sums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7a8de3",
   "metadata": {},
   "source": [
    "Note that, in my experience, I haven't had any documents (even any combined documents) which had more than on-the-order-of-10^3 words. That means that the original sum can be put into the algorithm. However, I thought there might be some insights found by looking at the partial sums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5790738",
   "metadata": {},
   "source": [
    "I looked at the [Euler-Maclaurin Formula](https://web.archive.org/web/20230827161607/https://en.wikipedia.org/wiki/Euler%E2%80%93Maclaurin_formula) in the form useful for asymptotic expansion of sums (The sum of the infinite version of our series, $\\sum_{n=0}^\\infty{\\frac{1}{n+\\beta}}$ diverges to positive infinity.)\n",
    "\n",
    "$\\sum_{n=a}^N {g(x)} \\sim \\int_a^N {g(x) dx} + \\frac{g(N) +g(a)}{2} + \\sum_{k=1}^\\infty {\\frac{B_{2k}}{(2k)!} \\left( g^\\left( 2k-1 \\right)(N) - g^\\left( 2k-1 \\right)(a) \\right)}$\n",
    "\n",
    "_Note that $\\sim$ means asymptotically approaches_\n",
    "\n",
    "where $B_p$ is the $p^{th}$ [Bernoulli polynomial](https://web.archive.org/web/20230827163846/https://en.wikipedia.org/wiki/Bernoulli_polynomials) and $g^\\left( q \\right)(z)$ is the $q^{th}$ derivative of the function, $g(z)$\n",
    "\n",
    "I was led to this by a [Q&A on Quora](https://www.quora.com/Whats-the-general-formula-to-find-the-partial-sum-of-any-series-f-n) which requires a free subscription and which does not allow the archive.org Wayback Machine to archive it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ecdda1",
   "metadata": {},
   "source": [
    "### Proportion (fraction) of appearances of top word to number of total words\n",
    "\n",
    "#### (Important formula)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4027a0ee",
   "metadata": {},
   "source": [
    "For a text with $N$ distinct words, the proportion/(fraction) of the word with rank, $r$ is\n",
    "\n",
    "$\\pi(r) = \\left[1 / \\left(\\kappa \\cdot N\\right) \\right] \\cdot \\left[1 / \\left(r+2.7\\right)\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3e652e",
   "metadata": {},
   "source": [
    "### Too much space needed for that\n",
    "\n",
    "Thank goodness (and [Stephen Wolfram](https://web.archive.org/web/20230827170213/https://en.wikipedia.org/wiki/Stephen_Wolfram) for [Wolfram|Alpha](wolframalpha.com/input?i=sum_%28n%3D0%29%5Einfinity+1%2F%28n+%2B+b%29)\n",
    "\n",
    "```\n",
    "alt_text_str = ( \"WolframAlpha computational intelligence. \"\n",
    "                 \"Entry: the sum from n equals zero to infinity \"\n",
    "                 \"of one divided by quantity n plus b end quantity. \"\n",
    "                 \"Answers include the following. \"\n",
    "                 \"Infinite sum: [The entry] diverges to infinity. \"\n",
    "                 \"Partial sum formula: the sum from n equals zero to \"\n",
    "                 \"k of one over quantity n plus beta equals psi \"\n",
    "                 \"super zero of quantity beta plus k plus one end \"\n",
    "                 \"quantity minus psi super zero of b where psi super \"\n",
    "                 \"n of x is the nth derivative of the digamma \"\n",
    "                 \"function\"\n",
    ")\n",
    "```\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"partial_sum_from_wolfram_alpha.jpg\"\n",
    "       alt=\"WolframAlpha computational intelligence. Entry: the sum from n equals zero to infinity of one divided by quantity n plus b end quantity. Answers include the following. Infinite sum: [The entry] diverges to infinity. Partial sum formula: the sum from n equals zero to k of one over quantity n plus beta equals psi super zero of quantity beta plus k plus one end quantity minus psi super zero of b where psi super n of x is the nth derivative of the digamma function\"\n",
    "       width=\"auto\">\n",
    "</div>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5da66a3",
   "metadata": {},
   "source": [
    "### Some hints for implementing that in `Python`. \n",
    "\n",
    "Thanks to John D. Cook, PhD for the information at [Gamma and related functions in SciPy](https://web.archive.org/web/20230827165703/https://www.johndcook.com/blog/gamma_python/) at https://www.johndcook.com/blog/gamma_python/\n",
    "\n",
    "> Psi and polygamma functions\n",
    "> The derivative of `log( Γ(z) )` is denoted `ψ(z)` and is implemented in the `psi` function. The `nth` derivative of `ψ(z)` is implemented in `psi(n, z)`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48b03f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "description_idx_to_show = 0 # For this version, can be 0,1,2,3\n",
    "\n",
    "n_desc_words_array = [0] * len(description_word_counts)\n",
    "\n",
    "for i in range(0, len(description_word_counts)):\n",
    "    n_desc_words_array[i] = \\\n",
    "      len(description_word_counts[i])\n",
    "##endof:  for i in range(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44cf84c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[325, 293, 273, 265]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_desc_words_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99edfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special \n",
    "  #  I needed to pip install scipy. I'll update \n",
    "  #+ the instructions and the environment.\n",
    "\n",
    "def max_probability_for_graph(number_of_distinct_words):\n",
    "    beta = 2.7\n",
    "    approx_sum = \\\n",
    "      special.psi(0, beta + k + 1) - special.psi(0, beta)\n",
    "    \n",
    "    print(f\"To check, approx_sum = {approx_sum}\")\n",
    "    \n",
    "    kappa = approx_sum / number_of_distinct_words\n",
    "    \n",
    "    calulated_fraction = \n",
    "    \n",
    "##endof:  \n",
    "\n",
    "def approx_max_prob_by_partial_sum_terms(number_of_distinct_words)\n",
    "    \n",
    "    beta = 2.7\n",
    "    term_by_term_sum = 0\n",
    "\n",
    "     \n",
    "     ##endof:  for k in range(number_of_distinct_words)\n",
    "# ##endof:  approx_max_probability_by_partial_sum_terms(<N-words>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78fb9a6",
   "metadata": {},
   "source": [
    "Anyway, look at the previous histogram or inspect the histogram you get below to make this following number appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# approx_max = \\\n",
    "#   approximate_max_probability_for_graph(\n",
    "#     number_of_distinct_words = n_desc_words_array[desc_fname_idx_to_show] \n",
    "# )\n",
    "error_bars_frac = 0.1\n",
    "\n",
    "# max_probability_for_graph_desc = \\\n",
    "#  approximate_max_probability_for_graph(325, 1)\n",
    "\n",
    "init_probability = 0.05\n",
    "\n",
    "max_probability_for_graph = init_probability + error_bars_frac * init_probability\n",
    "\n",
    "print(max_probability_for_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c9f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(description_word_counts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b257f482",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top_word_rank_in_desc = 1\n",
    "this_desc_idx = top_word_rank_in_desc\n",
    "\n",
    "this_corresponding_word = get_description_word_at_rank(top_word_rank_in_desc)\n",
    "\n",
    "print(f\"this_corresponding_word: {this_corresponding_word}\")\n",
    "\n",
    "rank_desc, _ = find_word_in_both_display_lists(\n",
    "                this_corresponding_word,\n",
    "                description_word_counts[desc_fname_idx_to_show],\n",
    "                application_word_counts[0],\n",
    "                   #  we only have one table - \n",
    "                   #+ it's at any legal index;\n",
    "                   #+ let's choose 0\n",
    "                name_of_display_list_1 = f\"description_word_counts[{this_desc_idx}]\",\n",
    "                name_of_display_list_2 = \"application_word_counts[0]\"\n",
    ")\n",
    "\n",
    "fig_filename_desc = \"\"\n",
    "\n",
    "if rank_desc is None:\n",
    "    import matplotlib.image as mpimg\n",
    "    fig_filename_desc = \"description_word_not_found.png\"\n",
    "    img = mpimg.imread(fig_filename_desc)\n",
    "    imgplot = plt.imshow(img)\n",
    "##endof:  if rank_desc\n",
    "else:\n",
    "    get_freq_histo_specific(\n",
    "            description_word_counts[desc_fname_idx_to_show],\n",
    "            rank_index_1=this_desc_idx,\n",
    "            n_surrounding_words=3,\n",
    "            do_show_word_and_count_lists=False,\n",
    "            ylim_top_val=max_probability_for_graph\n",
    "    )\n",
    "    \n",
    "    fig_filename_desc = (\n",
    "            f\"description_word_rank_{this_desc_idx}_\"\n",
    "            f\"desc_{desc_fname_idx_to_show}.png\"\n",
    "    )\n",
    "    \n",
    "    print(fig_filename_desc)\n",
    "\n",
    "    title_for_desc = (f\"Word frequency rank ({rank_desc}) and surrounding context in \"\n",
    "                      f\"job description for the word, {this_corresponding_word}\"\n",
    "                 )\n",
    "    plt.title(title_for_desc)\n",
    "\n",
    "    plt.savefig(fig_filename_desc,\n",
    "                bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "##endof:  if/else rank_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340abb8a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "_, rank_appl = find_word_in_both_display_lists(\n",
    "        this_corresponding_word,\n",
    "        description_word_counts[desc_fname_idx_to_show],\n",
    "        application_word_counts[0],\n",
    "           #  we only have one table - \n",
    "           #+ it's at any legal index;\n",
    "           #+ let's choose 0\n",
    "        name_of_display_list_1 = f\"description_word_counts[{this_desc_idx}]\",\n",
    "        name_of_display_list_2 = \"application_word_counts[0]\"\n",
    ")\n",
    "\n",
    "fig_filename_appl = \"\"\n",
    "\n",
    "if rank_appl is None:\n",
    "    import matplotlib.image as mpimg\n",
    "    fig_filename_appl = \"application_word_not_found.png\"\n",
    "    img = mpimg.imread(fig_filename_appl)\n",
    "    imgplot = plt.imshow(img)\n",
    "##endof:  if rank_desc\n",
    "else:\n",
    "    corresponding_index = rank_appl\n",
    "    \n",
    "    get_freq_histo_specific(application_word_counts[0],\n",
    "                            rank_index_1=corresponding_index,\n",
    "                            n_surrounding_words=3,\n",
    "                            do_show_word_and_count_lists=False,\n",
    "                            ylim_top_val=max_probability_for_graph\n",
    "    )\n",
    "\n",
    "    fig_filename_appl = (f\"application_word_rank_{corresponding_index}_\"\n",
    "                         f\"desc_{desc_fname_idx_to_show}.png\"\n",
    "                        )\n",
    "\n",
    "    title_for_appl = (f\"Word frequency rank ({rank_appl}) and surrounding context in \"\n",
    "                      f\"job application for the word, {this_corresponding_word}\"\n",
    "                     )\n",
    "    plt.title(title_for_appl)\n",
    "\n",
    "    plt.savefig(fig_filename_appl,\n",
    "            bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "##endof:  ##endof:  if/else rank_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ada010",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"  img src values for the two images:\")\n",
    "print(f'\"{fig_filename_desc}\"')\n",
    "print(f'\"{fig_filename_appl}\"')\n",
    "\n",
    "print()\n",
    "print(\"  img alt values for the two images:\")\n",
    "alt_text_1 = (f'\"Histogram for the word, {this_corresponding_word}, in '\n",
    "               'the job description text\"'\n",
    "             )\n",
    "alt_text_2 = (f'\"Histogram for the word, {this_corresponding_word}, in '\n",
    "               'the job application text\"'\n",
    "             )\n",
    "print(alt_text_1)\n",
    "print(alt_text_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a127c5",
   "metadata": {},
   "source": [
    "### Change the img src values and img alt values, then see the histograms together\n",
    "\n",
    "You might need to double-click on the image to get the html source. You also might need to click on the image (or image-not-found icon) just to make things reload.\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"description_word_rank_1_desc_0.png\"\n",
    "       alt=\"Histogram for the word, software, in the job description text\"\n",
    "       width=\"auto\">\n",
    "</div>\n",
    "<br/>\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"application_word_rank_2_desc_0.png\"\n",
    "       alt=\"Histogram for the word, software, in the job application text\"\n",
    "       width=\"auto\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d6df6b",
   "metadata": {},
   "source": [
    "## When I Get to Part 5 - Detail $\\forall$ 25 Top Words (Both) and Metrics\n",
    "\n",
    "After the dash, that means, \"Detail \\[for all\\] 25 Top Words (Both \\[Job Description and Job Application\\]) and Metrics.\n",
    "\n",
    "### Basically making things more complete\n",
    "\n",
    "I've got the basics of what I'll need and a bit more, but it feels weird not to have a metric. Part 5 will have a big image with lots of subplots to show the comparison of all 25 top description words and then any words in the application top 25 that won't have already shown up.\n",
    "\n",
    "I'm not sure exactly what will be needed, but let's go ahead with ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a0231",
   "metadata": {},
   "source": [
    "### Pickle time again\n",
    "\n",
    "Then, the link for the Part 5 MyBinder will be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeb9c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_filename_4_to_5 = \"important_part_4_vars.pkl\"\n",
    "\n",
    "things_to_pickle_4 = [\n",
    "    local_job_desc_filenames,\n",
    "    local_job_appl_filenames,\n",
    "    description_word_counts,\n",
    "    application_word_counts,\n",
    "    list_of_display_table_desc\n",
    "]\n",
    "\n",
    "with open(pickle_filename_4_to_5, 'wb') as pfh:\n",
    "    pickle.dump(things_to_pickle_4, pfh)\n",
    "##endof:  with open ... as pfh # (pickle file handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630cb4d4",
   "metadata": {},
   "source": [
    "[Part 5 On GitHub]()\n",
    "\n",
    "[Part 5 On MyBinder]()\n",
    "\n",
    "Or, alternatively/eventually, use the badge as a link for the MyBinder version.\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/bballdave025/job-app-word-freq/main?labpath=Part_05_NLPPresentationJobHunt_DemoWordFreq.ipynb)\n",
    "\n",
    "I'll be getting an image that shows the version and the part number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb62fb59",
   "metadata": {},
   "source": [
    "### Since we're at the end of what I've written now ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7ae60",
   "metadata": {},
   "source": [
    "## Output for Description and Application:\n",
    "\n",
    "### &lt;FILL THIS IN&gt;\n",
    "\n",
    "@TODO : I need to look at what I've done here in previous versions.\n",
    "\n",
    "### Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c55c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1acab5",
   "metadata": {},
   "source": [
    "The output when I actually did this was\n",
    "\n",
    "```\n",
    "1693128500_20230827T092820-0600\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e01967",
   "metadata": {},
   "source": [
    "## Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b13da",
   "metadata": {},
   "source": [
    "- Look at ranking, counts, percentage, etc. for FamilySearch's (job description's) top 25 words as found in my (job application's) word counts, then vice-versa. \n",
    "  - Code setup completed 2023-08-20. Putting all 25 in would make a very busy display, so I just did a few.\n",
    "- Get rid of words that are necessary for grammar, but which don't matter too much in determining whether the two documents match up. (Found term on 2023-08-07. It's \"stopwords\".)\n",
    "  - Completed 2023-08-09"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e7f734",
   "metadata": {},
   "source": [
    "**Some new future steps**\n",
    "\n",
    "- Do word counts for the pair of top 25, but then also do the fraction each word comprises of the whole (non-stopword) text.\n",
    "  - Completed sometime before 2023-08-27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73f0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
