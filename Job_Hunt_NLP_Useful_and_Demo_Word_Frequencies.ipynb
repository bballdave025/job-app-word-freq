{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02d10fc3",
   "metadata": {},
   "source": [
    "# Job-Hunt NLP Demo\n",
    "\n",
    "Which demo will also be useful in doing some quick NLP work to see how my résumé's word distribution matches that from job descriptions.\n",
    "\n",
    "There's a wonderful project out there, [MyBinder](https://mybinder.org), which allows you to interactively run a Jupyter notebook completely online. It's nice to have when you'd like to play with code and see better the outputs that come from running that code. I've had some problems with images going down, but I'm going to work to keep this one up.\n",
    "\n",
    "The link to the online, interactive notebook - the binder - will be at the badge you see right here\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/bballdave025/ancestry-freq/main?labpath=Ancestry_NLP_Useful_and_Demo.ipynb)\n",
    "\n",
    "when I figure out how to get the Binder to stay.\n",
    "\n",
    "<strike>Go ahead and give it a try!</strike> \\[Soon!\\]\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f69f9",
   "metadata": {},
   "source": [
    "## My setup from the Conda Prompt\n",
    "\n",
    "I'm in Windows for this one. I created a repo with the right name on GitHub, then\n",
    "\n",
    "```\n",
    "(base) <path>\\my_repos>git clone git@github.com:bballdave025/job-app-word-freq.git\n",
    "Cloning into 'job-app-word-freq'...\n",
    "Enter passphrase for key '/c/Users/bballdave025/.ssh/id_██':\n",
    "remote: Enumerating objects: 5, done.\n",
    "remote: Counting objects: 100% (5/5), done.\n",
    "remote: Compressing objects: 100% (5/5), done.\n",
    "remote: Total 5 (delta 0), reused 0 (delta 0), pack-reused 0\n",
    "Receiving objects: 100% (5/5), 14.02 KiB | 1.56 MiB/s, done.\n",
    "\n",
    "(base) <path>\\my_repos>cd job-app-word-freq\n",
    "\n",
    "(base) <path>\\my_repos\\job-app-word-freq>dir\n",
    " Volume in drive C is OS\n",
    " Volume Serial Number is ████-████\n",
    "\n",
    " Directory of <path>\\my_repos\\job-app-word-freq\n",
    "\n",
    "07/22/2023  06:04 PM    <DIR>          .\n",
    "07/22/2023  06:04 PM    <DIR>          ..\n",
    "07/22/2023  06:04 PM             3,078 .gitignore\n",
    "07/22/2023  06:04 PM            35,149 LICENSE\n",
    "07/22/2023  06:04 PM                93 README.md\n",
    "               3 File(s)         38,320 bytes\n",
    "               2 Dir(s)  751,770,595,328 bytes free\n",
    "\n",
    "(base) <path>\\job-app-word-freq>::  After some dinner, let's see\n",
    "(base) <path>\\job-app-word-freq>::+ how quick and reckless I can\n",
    "(base) <path>\\job-app-word-freq>::+ be in setting up this word-\n",
    "(base) <path>\\job-app-word-freq>::+ frequency analyzer for job\n",
    "(base) <path>\\job-app-word-freq>::+ applications\n",
    "(base) >\n",
    "(base) >::  I'm in the Anaconda Prompt (miniconda3)\n",
    "(base) >\n",
    "(base) > ^\n",
    "More? powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") ^\n",
    "More? -replace '[.][0-9]*_', '_'\n",
    "1690108602_20230723T103642-0600\n",
    "\n",
    "(base) >\n",
    "(base) >::  Or, instead of after dinner,\n",
    "(base) >::+ after the whole day.\n",
    "(base) >\n",
    "(base) >::  Q&R\n",
    "(base) >\n",
    "(base) >\"C:\\Program Files (x86)\\Notepad++\\notepad++.exe\" .gitignore\n",
    "\n",
    "(base) >\n",
    "(base) >git status\n",
    "On branch main\n",
    "Your branch is up to date with 'origin/main'.\n",
    "\n",
    "Changes not staged for commit:\n",
    "  (use \"git add <file>...\" to update what will be committed)\n",
    "  (use \"git restore <file>...\" to discard changes in working directory)\n",
    "        modified:   .gitignore\n",
    "\n",
    "Untracked files:\n",
    "  (use \"git add <file>...\" to include in what will be committed)\n",
    "        .gitattributes\n",
    "\n",
    "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
    "\n",
    "(base) >git branch\n",
    "* main\n",
    "\n",
    "(base) >:: Not best practice, but Q&R\n",
    "(base) >git add --all\n",
    "\n",
    "(base) >git commit -m \"Template .gitignore, moved .gitattributes comments to end\"\n",
    "[main 220d76f] Template .gitignore, moved .gitattributes comments to end\n",
    " 2 files changed, 102 insertions(+)\n",
    " create mode 100644 .gitattributes\n",
    "\n",
    "(base) >git push\n",
    "Enter passphrase for key '/c/Users/bballdave025/.ssh/id_██':\n",
    "Enumerating objects: 6, done.\n",
    "Counting objects: 100% (6/6), done.\n",
    "Delta compression using up to 8 threads\n",
    "Compressing objects: 100% (4/4), done.\n",
    "Writing objects: 100% (4/4), 1.89 KiB | 1.89 MiB/s, done.\n",
    "Total 4 (delta 1), reused 0 (delta 0), pack-reused 0\n",
    "remote: Resolving deltas: 100% (1/1), completed with 1 local object.\n",
    "To github.com:bballdave025/job-app-word-freq.git\n",
    "   829e9e7..220d76f  main -> main\n",
    "\n",
    "(base) >\n",
    "(base) >:: Let's set up our\n",
    "(base) >::+ conda environment.\n",
    "(base) >\n",
    "(base) >dir\n",
    " Volume in drive C is OS\n",
    " Volume Serial Number is ████-████\n",
    "\n",
    " Directory of <path>\\my_repos\\job-app-word-freq\n",
    "\n",
    "07/23/2023  10:42 AM    <DIR>          .\n",
    "07/23/2023  10:42 AM    <DIR>          ..\n",
    "07/23/2023  10:47 AM             2,341 .gitattributes\n",
    "07/23/2023  10:47 AM             3,942 .gitignore\n",
    "07/22/2023  06:04 PM            35,149 LICENSE\n",
    "07/22/2023  06:04 PM                93 README.md\n",
    "               4 File(s)         41,525 bytes\n",
    "               2 Dir(s)  751,267,094,528 bytes free\n",
    "\n",
    "(base) >git checkout -b quick-word-freq\n",
    "Switched to a new branch 'quick-word-freq'\n",
    "\n",
    "(base) >python -m pip install --upgrade pip\n",
    "Defaulting to user installation because normal site-packages is not writeable\n",
    "Requirement already satisfied: pip in c:\\programdata\\miniconda3\\lib\\site-packages (23.0.1)\n",
    "Collecting pip\n",
    "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
    "     ---------------------------------------- 2.1/2.1 MB 7.4 MB/s eta 0:00:00\n",
    "Installing collected packages: pip\n",
    "  WARNING: The scripts pip.exe, pip3.10.exe and pip3.exe are installed in 'C:\\Users\\bballdave025\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
    "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
    "Successfully installed pip-23.2.1\n",
    "\n",
    "(base) >:: Quick and Reckless; Q&R; not using admin here\n",
    "(base) >\n",
    "(base) > ^\n",
    "More? conda create -y --name job_freak python=3.10 ^\n",
    "More? numpy matplotlib pandas pathlib requests beautifulsoup4 jupyter\n",
    "```\n",
    "\n",
    "This will take a little while, and there will be\n",
    "\n",
    "`    :: LOTS OF OUTPUT                                                              `\n",
    "\n",
    "to go along with it, which output will end with\n",
    "\n",
    "`    Executing transaction: done                                                    `<br/>\n",
    "`    #                                                                              `<br/>\n",
    "`    # To activate this environment, use                                            `<br/>\n",
    "`    #                                                                              `<br/>\n",
    "`    #    `<code>&#x24;</code>` conda activate job_freak                                                `<br/>\n",
    "`    #                                                                              `<br/>\n",
    "`    # To deactivate an active environment, use                                     `<br/>\n",
    "`    #                                                                              `<br/>\n",
    "`    #    `<code>&#x24;</code>` conda deactivate                                                        `<br/>\n",
    "`    #                                                                              `<br/>\n",
    "```\n",
    "\n",
    "(base) >\n",
    "(base) >conda activate job_freak\n",
    "```\n",
    "\n",
    "Note that I'm going to make an `environment.yml` file based on the output from two `conda env export` commands\n",
    "\n",
    "```\n",
    "(job_freak) >conda env export --from-history\n",
    "name: job_freak\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - numpy\n",
    "  - jupyter\n",
    "  - pathlib\n",
    "  - beautifulsoup4\n",
    "  - python=3.10\n",
    "  - matplotlib\n",
    "  - pandas\n",
    "  - requests\n",
    "prefix: C:\\Users\\bballdave025\\.conda\\envs\\job_freak\n",
    "\n",
    "(job_freak) >conda env export --from-history > environment.yml\n",
    "\n",
    "(job_freak) >conda env export > environment_details.yml\n",
    "```\n",
    "\n",
    "That second one will allow me to put version information that otherwise wouldn't be available in the first one. \n",
    "\n",
    "Editing `environment.yml` using information from `environment_details.yml`\n",
    "\n",
    "That will be useful for MyBinder. So cool, MyBinder!\n",
    "\n",
    "I always keep the `environment_details.yml` file, ever since the issues with stand-alone `keras` not being maintained, but it still being part of a part of a lot of my `environment.yml` files; before, environment.yml had `keras=<version>`; now, we get `tensorflow=<version>`. Made for some interesting docker debugging.\n",
    "\n",
    "\n",
    "```\n",
    "(job_freak) >:: We end up with\n",
    "(job_freak) >type environment.yml\n",
    "name: job_freak\n",
    "channels:\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - numpy=1.25\n",
    "  - jupyter=1.0.0\n",
    "  - pathlib=1.0.1\n",
    "  - beautifulsoup4=4.12.2\n",
    "  - python=3.10.12\n",
    "  - matplotlib=3.7.1\n",
    "  - pandas=1.5.3\n",
    "  - requests=2.29.0\n",
    "  - pip=23.0.1\n",
    "prefix: C:\\Users\\bballdave025\\.conda\\envs\\job_freak\n",
    "\n",
    "(job_freak) >\n",
    "(job_freak) >:: Last steps before going to church\n",
    "(job_freak) >\n",
    "(job_freak) >jupyter notebook\n",
    "```\n",
    "\n",
    "In the main page, click \"New\" > \"Notebook: Python3 (ipykernel)\", then to \"File\" > \"Save as...\". Save it. Copy all this stuff in here to the notebook.\n",
    "\n",
    "Switching to another `Anaconda Prompt (miniconda3)`\n",
    "\n",
    "```\n",
    "(base) >cd C:\\David\\my_repos\\job-app-word-freq\n",
    "<path>\\job-app-word-freq>conda activate job_freak\n",
    "(job_freak) >git status\n",
    "On branch quick-word-freq\n",
    "Untracked files:\n",
    "  (use \"git add <file>...\" to include in what will be committed)\n",
    "        Job_Hunt_NLP_Useful_and_Demo_Word_Frequencies.ipynb\n",
    "        Untitled.ipynb\n",
    "        environment.yml\n",
    "        environment_details.yml\n",
    "\n",
    "nothing added to commit but untracked files present (use \"git add\" to track)\n",
    "\n",
    "(job_freak) >git add --all\n",
    "\n",
    "(job_freak) >git commit -m \"Notebook started\"\n",
    "[quick-word-freq 1b12909] Notebook started\n",
    " 4 files changed, 521 insertions(+)\n",
    " create mode 100644 Job_Hunt_NLP_Useful_and_Demo_Word_Frequencies.ipynb\n",
    " create mode 100644 Untitled.ipynb\n",
    " create mode 100644 environment.yml\n",
    " create mode 100644 environment_details.yml\n",
    "\n",
    "(job_freak) >git push\n",
    "fatal: The current branch quick-word-freq has no upstream branch.\n",
    "To push the current branch and set the remote as upstream, use\n",
    "\n",
    "    git push --set-upstream origin quick-word-freq\n",
    "\n",
    "To have this happen automatically for branches without a tracking\n",
    "upstream, see 'push.autoSetupRemote' in 'git help config'.\n",
    "\n",
    "\n",
    "(job_freak) >git push --set-upstream origin quick-word-freq\n",
    "\n",
    "(job_freak) >\n",
    "(job_freak) > ^\n",
    "More? powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") ^\n",
    "More? -replace '[.][0-9]*_', '_'\n",
    "1690113083_20230723T115123-0600\n",
    "\n",
    "(job_freak) >\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0d3fc8",
   "metadata": {},
   "source": [
    "## Back on Day 3\n",
    "\n",
    "### How much time so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c85596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65277b5",
   "metadata": {},
   "source": [
    "For right now, that gave:\n",
    "\n",
    "`1690197792_20230724T112312-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2e02c",
   "metadata": {},
   "source": [
    "## Import for how-much-time updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd63ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# RUN THIS ONE EACH TIME (AS WELL AS THE IMPORT ABOVE)\n",
    "#####\n",
    "\n",
    "# Find how long I've been working so far\n",
    "\n",
    "import math\n",
    "\n",
    "def time_total(timestamp_pairs_beg_and_end):\n",
    "    total_seconds = 0\n",
    "    for timestamp_pair in timestamp_pairs_beg_and_end:\n",
    "        begin_timestamp = timestamp_pair[0]\n",
    "        end_timestamp = timestamp_pair[1]\n",
    "        total_seconds += abs(end_timestamp - begin_timestamp)\n",
    "          ## Absolute value in case pair begin & end switched\n",
    "    ##endof:  for timestamp_pair in timestamp_pairs_beg_and_end\n",
    "    print(f\"total_seconds: {total_seconds}\")\n",
    "    n_seconds_per_minute = 60\n",
    "    total_minutes_fractional, total_minutes_integer = \\\n",
    "                math.modf(total_seconds / n_seconds_per_minute)\n",
    "    print(f\"total_minutes: {total_minutes_fractional + total_minutes_integer}\")\n",
    "    print(f\"{int(total_minutes_integer)} minutes \"\n",
    "          f\"and \"\n",
    "          f\"{round(total_minutes_fractional * n_seconds_per_minute)} seconds\")\n",
    "    n_minutes_per_hour = 60\n",
    "    total_hours_fractional, total_hours_integer = \\\n",
    "        math.modf(total_seconds / n_seconds_per_minute / n_minutes_per_hour)\n",
    "    print(f\"total_hours: {total_hours_integer + total_hours_fractional}\")\n",
    "    mins_after_hrs_frac, mins_after_hrs_int = \\\n",
    "        math.modf(total_hours_fractional * n_minutes_per_hour)\n",
    "    print(f\"{round(mins_after_hrs_frac + mins_after_hrs_int, 2)} minutes\"\n",
    "          f\" more than {int(total_hours_integer)} hours\")\n",
    "    n_sec_after_hm = round(mins_after_hrs_frac * n_seconds_per_minute)\n",
    "    print(f\"{int(total_hours_integer)} hours and \"\n",
    "          f\"{int(mins_after_hrs_int)} minutes and \"\n",
    "          f\"{n_sec_after_hm} seconds\")\n",
    "##endof:  time_total(timestamp_pairs_beg_and_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405654c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# beg_and_end_timestamps = [[1690108602, 1690113083],]\n",
    "# \n",
    "# time_total(beg_and_end_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785383af",
   "metadata": {},
   "source": [
    "That gives me\n",
    "\n",
    "```\n",
    "total_seconds: 4481\n",
    "total_minutes: 74.68333333333334\n",
    "74 minutes and 41 seconds\n",
    "total_hours: 1.2447222222222223\n",
    "14.68 minutes more than 1 hours\n",
    "1 hours and 14 minutes and 41 seconds\n",
    "```\n",
    "\n",
    "at\n",
    "\n",
    "`1690197792_20230724T112312-0600`\n",
    "\n",
    "I'd like to get it done in two hours. What came before was prep. Where I am with time will determine whether I count that or not.\n",
    "\n",
    "The texts of my résumé, cover letter, and maybe previous job descriptions (probably after the original work) are in the local directory as (filenames will come after pasting and having lunch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c23f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a0e2a",
   "metadata": {},
   "source": [
    "Right now, that gives\n",
    "\n",
    "`1690198634_20230724T113714-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda6f3ab",
   "metadata": {},
   "source": [
    "And back after scripture study (Acts 16-20 start)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19df299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd928669",
   "metadata": {},
   "source": [
    "Right now, we get\n",
    "\n",
    "`1690202804_20230724T124644-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7621ae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# beg_and_end_timestamps = [[1690108602, 1690113083],[1690197792, 1690198634],]\n",
    "# \n",
    "# time_total(beg_and_end_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336ab5a",
   "metadata": {},
   "source": [
    "Now, at `1690202804_20230724T124644-0600`, we get\n",
    "\n",
    "```\n",
    "total_seconds: 5323\n",
    "total_minutes: 88.71666666666667\n",
    "88 minutes and 43 seconds\n",
    "total_hours: 1.478611111111111\n",
    "28.72 minutes more than 1 hours\n",
    "1 hours and 28 minutes and 43 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2201e56",
   "metadata": {},
   "source": [
    "## Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d411d8",
   "metadata": {},
   "source": [
    "The texts of my résumé, cover letter, and maybe previous job descriptions (probably on a second run-through) have been copied and pasted into the following, local text files.\n",
    "\n",
    "```\n",
    "BLACK-David_Resume_FS_SrSoftwareDevEngineer_2023-07-22.txt\n",
    "BLACK-David_CoverLetter_FS_SrSoftwareDevEngineer_2023-07-22.txt\n",
    "```\n",
    "\n",
    "And I'm doing Q&R to not fix it any more.\n",
    "\n",
    "The job application is at\n",
    "\n",
    "https://epej.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1001/job/356975/?utm_medium=jobshare\n",
    "\n",
    "and archived at\n",
    "\n",
    "https://web.archive.org/web/20230724185343/https://epej.fa.us2.oraclecloud.com/hcmUI/CandidateExperience/en/sites/CX_1001/job/356975/\n",
    "\n",
    "I'll pull from the archived version, so there is no problem with anyone trying to reproduce this. I also won't worry about the similar jobs listed at the bottom. Quick & Reckless (Q&R)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b18797",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_text_filenames = \\\n",
    "  [\"BLACK-David_Resume_FS_SrSoftwareDevEngineer_2023-07-22.txt\",\n",
    "   \"BLACK-David_CoverLetter_FS_SrSoftwareDevEngineer_2023-07-22.txt\"\n",
    "  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92a73f7",
   "metadata": {},
   "source": [
    "### Import statements for getting job-description text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b82fb3",
   "metadata": {},
   "source": [
    "Started this, but ran into problem with the job description website pulling from a database. I'll paste it and complete it, further down.\n",
    "\n",
    "```\n",
    "import requests\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "import shutil\n",
    "\n",
    "import math # again, for checking amount of time used.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import UnicodeDammit\n",
    "\n",
    "def clean_text_string_quickly(input_str):\n",
    "    processing_str = input_str\n",
    "    \n",
    "    ## get rid of punctuation\n",
    "    \n",
    "    \n",
    "    ## get everything on one line\n",
    "    \n",
    "    ## get spacing right\n",
    "    \n",
    "    ## word replacements for common (found) contractions\n",
    "    \n",
    "    ## word replacements for numbers - especially found numbers\n",
    "    \n",
    "    \n",
    "##endof:  clean_text_string_quickly(input_str)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb355d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_job_desc_filename = \"Job_Description_SrSoftwareDevEngineer_2023-07-24.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a90a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Try 1 - from the archive\n",
    "\n",
    "# Let's test download, html parsing, and find contractions, numbers\n",
    "\n",
    "source_url = ( r\"https://web.archive.org/web/20230724185343/\"\n",
    "               r\"https://epej.fa.us2.oraclecloud.com/hcmUI/\"\n",
    "               r\"CandidateExperience/en/sites/CX_1001/job/356975/\"\n",
    "             )\n",
    "\n",
    "source_url = ( r\"https://epej.fa.us2.oraclecloud.com/hcmUI/\"\n",
    "               r\"CandidateExperience/en/sites/CX_1001/job/356975/\"\n",
    "             )\n",
    "\n",
    "complete_html_code_response = requests.get(source_url)\n",
    "\n",
    "print(f\"resp: {str(complete_html_code_response)}\")\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "complete_html_as_text = complete_html_code_response.text\n",
    "\n",
    "print(f\"as text:\\n----------\\n{str(complete_html_as_text)}\\n----------\")\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "soup_cleaner = BeautifulSoup(complete_html_as_text, 'html.parser')\n",
    "\n",
    "# Check encoding (and hope it starts out as utf-8)\n",
    "print(f\"original encoding: {str(soup_cleaner.original_encoding)}\")\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "job_desc_init_str = soup_cleaner.body.get_text()\n",
    "\n",
    "print(f\"job_desc_init_str:\\n----------\\n{str(job_desc_init_str)}\\n----------\")\n",
    "print()\n",
    "print()\n",
    "print()\n",
    "\n",
    "print(\"######################################################\")\n",
    "print()\n",
    "print(\"Looks like JavaScript, ajax, etc.\")\n",
    "print(\"Let's try without the archive.org, but I doubt we'll get a difference.\")\n",
    "print(\"Actually, Q&R. Rather than looking through the code and finding what\")\n",
    "print(\"gets pulled from the database, I'm just going to copy/paste the text\")\n",
    "print(f\"into {local_job_desc_filename}\")\n",
    "print()\n",
    "print(\"I'm pretty sure I'm over the original two hour goal, but we can\")\n",
    "print(\"say that getting the Jupyter Notebook ready wasn't part, so the\")\n",
    "print(\"total can go to 3h15m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ca5e0c",
   "metadata": {},
   "source": [
    "\\######################################################\n",
    "\n",
    "Looks like JavaScript, ajax, etc.\n",
    "\n",
    "Let's try without the archive.org, but I doubt we'll get a difference.\n",
    "\n",
    "Actually, Q&R. Rather than looking through the code and finding what gets pulled from the database, I'm just going to copy/paste the text into `local_job_desc_filename = \"Job_Description_SrSoftwareDevEngineer_2023-07-24.txt\"`\n",
    "\n",
    "I'm pretty sure I'm over the original two hour goal, but we can say that getting the Jupyter Notebook ready wasn't part, so the \n",
    "total can go to 3h15m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec268cb5",
   "metadata": {},
   "source": [
    "## Copying and pasting\n",
    "\n",
    "### Rather than building a webscraper again or following the script code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1d2c8f",
   "metadata": {},
   "source": [
    "I'll now check my total time, then take a productivity-driving break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1909c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef1f28",
   "metadata": {},
   "source": [
    "Right now, we get\n",
    "\n",
    "`1690206455_20230724T134735-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c424bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# beg_and_end_timestamps = [[1690108602, 1690113083],[1690197792, 1690198634],[1690202804, 1690206455],]\n",
    "# \n",
    "# time_total(beg_and_end_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc643c",
   "metadata": {},
   "source": [
    "Before the break, our output was\n",
    "\n",
    "```\n",
    "total_seconds: 8974\n",
    "total_minutes: 149.56666666666666\n",
    "149 minutes and 34 seconds\n",
    "total_hours: 2.4927777777777775\n",
    "29.57 minutes more than 2 hours\n",
    "2 hours and 29 minutes and 34 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d042c4",
   "metadata": {},
   "source": [
    "## Getting and Cleaning Docs for Word Counts\n",
    "\n",
    "I'm back. I'll check in, then `git` everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c745322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bafb40",
   "metadata": {},
   "source": [
    "At this point, our response is\n",
    "\n",
    "`1690220846_20230724T174726-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c57519",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_job_desc_filenames = [local_job_desc_filename]\n",
    "local_job_app_filenames = application_text_filenames\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(local_job_desc_filenames)\n",
    "pprint.pprint(local_job_app_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706b076",
   "metadata": {},
   "source": [
    "Output was\n",
    "\n",
    "```\n",
    "['Job_Description_SrSoftwareDevEngineer_2023-07-24.txt']\n",
    "['BLACK-David_Resume_FS_SrSoftwareDevEngineer_2023-07-22.txt',\n",
    " 'BLACK-David_CoverLetter_FS_SrSoftwareDevEngineer_2023-07-22.txt']\n",
    "```\n",
    "\n",
    "at `1690220846_20230724T174726-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in texts to original strings\n",
    "\n",
    "complete_description_text = \" \"\n",
    "complete_application_text = \" \"\n",
    "\n",
    "for this_description_filename in local_job_desc_filenames:\n",
    "    with open(this_description_filename, 'r', encoding='utf-8') as dfh:\n",
    "        complete_description_text += dfh.read()\n",
    "    ##endof:  with open ... dfh\n",
    "##endof:  for this_description_filename in local_job_desc_filenames\n",
    "\n",
    "for this_application_filename in local_job_app_filenames:\n",
    "    with open(this_application_filename, 'r', encoding='utf-8') as afh:\n",
    "        complete_application_text += afh.read()\n",
    "    ##endof:  with open ... afh\n",
    "##endof:  for this_application_filename in local_job_app_filenames\n",
    "\n",
    "complete_description_text += \" \"\n",
    "complete_application_text += \" \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e6c18",
   "metadata": {},
   "source": [
    "### Expanding on our cleaning code from above\n",
    "\n",
    "We will iterate a bit, so as not to have to write a text normalizer for the whole world. Rather than putting together regexes to test for things like which contractions are there and which other things might need changing (especially things like dashes), I'm doing simple regexes. Q&R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c83bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#import html\n",
    "import re\n",
    "import string\n",
    "#import shutil\n",
    "\n",
    "#from bs4 import BeautifulSoup\n",
    "#from bs4 import UnicodeDammit\n",
    "\n",
    "def clean_text_string_quickly(input_str):\n",
    "    processing_str = input_str\n",
    "    \n",
    "    ## one line, single-spaced\n",
    "    processing_str = ' '.join(processing_str.split())\n",
    "    processing_str = processing_str.replace(\"\\t\", \" \")\n",
    "    processing_str = processing_str.replace(\"\\n\", \" \")\n",
    "    processing_str = re.sub(r\"([^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## get rid of outside-ascii (or control character)\n",
    "    processing_str = re.sub(r\"[^\\u0020-\\u007E]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## my stuff, before looking through the clean texts\n",
    "    processing_str = re.sub(r\"[ ][|]+[ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    processing_str = processing_str.replace(r\"&\", \"and\")\n",
    "    processing_str = processing_str.replace(r\"U.S.\", \"U S \")\n",
    "    \n",
    "    ## get rid of punctuation\n",
    "    processing_str = re.sub(r\"(([^0-9 ])[.,!?:\\\"']([) ]|$))\",\n",
    "                            r\"\\g<2>\\g<3>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    processing_str = re.sub(r\"(([0-9 ])[.,!?:\\\"']([ ]|$))\",\n",
    "                            r\"\\g<2>\\g<3>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    # parentheses\n",
    "    processing_str = processing_str.replace(r\"(\", \" \")\n",
    "    processing_str = processing_str.replace(r\")\", \" \")\n",
    "    # dashes\n",
    "    processing_str = re.sub(r\"[ ][-]+[ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ##  lowercase - to skip until a few iterations through\n",
    "    ##+ cleaning the text\n",
    "    processing_str = processing_str.casefold()\n",
    "    \n",
    "    ## fixes found by iterating this cleaning function\n",
    "    processing_str = re.sub(r\"[ ][/][ ]\",\n",
    "                            \" \",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    processing_str = processing_str.replace(r\"monitors/equipment\", \n",
    "                                                  \"monitors equipment\")\n",
    "    processing_str = processing_str.replace(r\"notice/more\", \n",
    "                                                  \"notice more\")\n",
    "    \n",
    "    ##spacing fix at the end\n",
    "    processing_str = re.sub(r\"([^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            processing_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    ## Let's give it back\n",
    "    return processing_str\n",
    "\n",
    "##endof:  clean_text_string_quickly(input_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa9f136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll usually keep these two.\n",
    "do_look_at_description_text = True\n",
    "do_look_at_application_text = True\n",
    "\n",
    "#  this one can go (False) if you don't want the big strings\n",
    "#+ i.e. you don't want the complete file contents\n",
    "do_print_the_big_strings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98b8eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_description_text:\n",
    "    test1 = clean_text_string_quickly(complete_description_text)\n",
    "    if do_print_the_big_strings:\n",
    "        print(test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    test2 = clean_text_string_quickly(complete_application_text)\n",
    "    if do_print_the_big_strings:\n",
    "        print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcfaa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at contractions\n",
    "if do_look_at_description_text:\n",
    "    print(re.findall(r\"\\b('[\\w']+\\b|[\\w']+'[\\w']+|[\\w']+')\\b\", test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e55de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    print(re.findall(r\"\\b('[\\w']+\\b|[\\w']+'[\\w']+|[\\w']+')\\b\", test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c271e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at all slashes\n",
    "if do_look_at_description_text:\n",
    "    print(re.findall(r\"\\b[\\w/]+/[\\w/]+\\b\", test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eaeff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_look_at_application_text:\n",
    "    print(re.findall(r\"\\b[\\w./]+/[\\w/]+\\b\", test2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be314ab0",
   "metadata": {},
   "source": [
    "It was good to make the search for things to clean simpler. After a break/dinner/nap, It'll be time to do the actual frequency counts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2708e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f51c483",
   "metadata": {},
   "source": [
    "With the cleaning stuff done above, we are at\n",
    "\n",
    "`1690223774_20230724T183614-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e06767",
   "metadata": {},
   "source": [
    "And my total time for now,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf2c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# beg_and_end_timestamps = [[1690108602, 1690113083], \\\n",
    "#                           [1690197792, 1690198634], \\\n",
    "#                           [1690202804, 1690206455],\n",
    "#                           [1690220846, 1690223774],]\n",
    "# \n",
    "# time_total(beg_and_end_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd304707",
   "metadata": {},
   "source": [
    "Here and now, at `1690223774_20230724T183614-0600`, we get\n",
    "\n",
    "```\n",
    "total_seconds: 11902\n",
    "total_minutes: 198.36666666666667\n",
    "198 minutes and 22 seconds\n",
    "total_hours: 3.3061111111111114\n",
    "18.37 minutes more than 3 hours\n",
    "3 hours and 18 minutes and 22 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4948a164",
   "metadata": {},
   "source": [
    "Well, I'm at the limit (the time limit I mentioned, earlier), but I've got most of the hard stuff done. I just need to do the counts. That will be in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4593ac2f",
   "metadata": {},
   "source": [
    "## Word Frequency Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990a85f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e821de3a",
   "metadata": {},
   "source": [
    "Now, before beginning the actual word-counts, we get\n",
    "\n",
    "`1690279068_20230725T095748-0600`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2f8fb1",
   "metadata": {},
   "source": [
    "I want to use an `OrderedDict`, rather than mess with sorting the contents of a `dict`. Q&R. I ran the install command, but I now realize `collections` is a built-in module. Hooray!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2aafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "from collections import OrderedDict\n",
    "\n",
    "def get_sorted_word_counts(*cleaned_strings):\n",
    "                           #,\n",
    "                           #do_output_sorted_file=False,\n",
    "                           #sorted_filename=\\\n",
    "                           #    \"sorted_words_from_strings.txt\"):\n",
    "    '''\n",
    "    @return  OrderedDict\n",
    "    '''\n",
    "    \n",
    "    EXIT_NOWORDSWEREFOUND = -1\n",
    "    \n",
    "    work_with_str = combine_strings(cleaned_strings)\n",
    "    \n",
    "    list_of_words_in_str = work_with_str.split()\n",
    "    \n",
    "    if len(list_of_words_in_str) <= 0:\n",
    "        print(\"No words were found.\", file=sys.stdout)\n",
    "        print(\"The program will exit.\", file=sys.stdout)\n",
    "        #sys.exit(EXIT_NOWORDSWEREFOUND)\n",
    "        return EXIT_NOWORDSWEREFOUND\n",
    "    ##endof:  if len(list_of_words_in_str) <= 0\n",
    "    \n",
    "    word_count_ordered_dict = OrderedDict()\n",
    "    \n",
    "    for this_word in list_of_words_in_str:\n",
    "        if this_word in word_count_ordered_dict:\n",
    "            word_count_ordered_dict[this_word] += 1\n",
    "        else:\n",
    "            word_count_ordered_dict[this_word] = 1\n",
    "        ##endof:  if/else this_word in list_of_words_in_str\n",
    "    ##endof:  for this_word in list_of_words_in_str\n",
    "    \n",
    "    ## DWB note ##\n",
    "    ##  At this point, the OrderedDict is sorted by the\n",
    "    ##+ order in which keys were inserted, not by their\n",
    "    ##+ count.\n",
    "    \n",
    "    for key, _ in \\\n",
    "          sorted(word_count_ordered_dict.items(),\n",
    "                 key=lambda word_and_count: word_and_count[1],\n",
    "                 reverse=True):\n",
    "        word_count_ordered_dict.move_to_end(key)\n",
    "    ##endof:  for myword, _ ...\n",
    "    \n",
    "    return word_count_ordered_dict\n",
    "    \n",
    "##endof:  get_sorted_word_counts(*cleaned_strings)\n",
    "\n",
    "def combine_strings(tuple_of_strings):\n",
    "                    #, \n",
    "                    #do_output_raw_file=False,\n",
    "                    #raw_filename='raw_words_from_strings.txt'):\n",
    "    '''\n",
    "    @return  string\n",
    "    '''\n",
    "    \n",
    "    returned_str = \" \"\n",
    "    \n",
    "    for this_str in tuple_of_strings:\n",
    "        returned_str += this_str + \" \"\n",
    "    ##endof:  for this_str in tuple_of_string\n",
    "    \n",
    "    ## one line, single-spaced\n",
    "    returned_str = ' '.join(returned_str.split())\n",
    "    returned_str = returned_str.replace(\"\\t\", \" \")\n",
    "    returned_str = returned_str.replace(\"\\n\", \" \")\n",
    "    returned_str = re.sub(r\"([^ ])[ ][ ]+($|[^ ])\",\n",
    "                            r\"\\g<1> \\g<2>\",\n",
    "                            returned_str,\n",
    "                            flags=re.IGNORECASE\n",
    "                           )\n",
    "    \n",
    "    return returned_str\n",
    "##endof:  combine_strings(tuple_of_strings)\n",
    "\n",
    "# @TODO: add a sort-by-word as well as sort-by-count flag\n",
    "# @TODO:  also, print out the pre-sorted and sorted files\n",
    "#       + with word lists, frequency, and in-order-of-\n",
    "#       + highest-count stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa30af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "do_print_long_full_version = False\n",
    "\n",
    "description_str = clean_text_string_quickly(complete_description_text)\n",
    "application_str = clean_text_string_quickly(complete_application_text)\n",
    "\n",
    "description_word_counts = \\\n",
    "           get_sorted_word_counts(description_str)\n",
    "application_word_counts = \\\n",
    "           get_sorted_word_counts(application_str)\n",
    "\n",
    "\n",
    "if do_print_long_full_version:\n",
    "    dashes=\"------------------------------------------------------------\"\n",
    "    short_dashes=\"-----\"\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print(dashes)\n",
    "    print(\" For the job description:\")\n",
    "    print(short_dashes)\n",
    "    pprint.pprint(description_word_counts)\n",
    "    print(dashes)\n",
    "    print()\n",
    "    print()\n",
    "    print(dashes)\n",
    "    print(\" For the job application materials (résumé, cover letter, etc.):\")\n",
    "    print(short_dashes)\n",
    "    pprint.pprint(description_word_counts)\n",
    "    print(dashes)\n",
    "    print()\n",
    "    print()\n",
    "##endof:  if do_print_long_full_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try_simpler = False\n",
    "\n",
    "##  We will do\n",
    "table_version_desc = [[\"desc_word\", \"desc_cnt\", \"desc_rank\"],]\n",
    "table_version_appl = [[\"appl_word\", \"appl_cnt\", \"appl_rank\"],]\n",
    "table_version_both = [[\"rank\", \"desc_word\", \"desc_cnt\", \n",
    "                       \"appl_word\", \"appl_cnt\"],\n",
    "                     ]\n",
    "\n",
    "description_items = list(description_word_counts.items())\n",
    "application_items = list(application_word_counts.items())\n",
    "\n",
    "n_words_description = len(description_items)\n",
    "n_words_application = len(application_items)\n",
    "\n",
    "print()\n",
    "print(\"NOTE THAT THESE ARE THE NUMBERS OF DISTINCT WORDS\")\n",
    "print(f\"n_words_description = {str(n_words_description)}\")\n",
    "print(f\"n_words_application = {str(n_words_application)}\")\n",
    "print()\n",
    "print()\n",
    "\n",
    "if try_simpler:\n",
    "    import pprint\n",
    "    print()\n",
    "    print()\n",
    "    print(\"DESCRIPTION\")\n",
    "    pprint.pprint(description_items)\n",
    "    print()\n",
    "    print(f\"n_words_description = {str(n_words_description)}\")\n",
    "    print()\n",
    "    print()\n",
    "    print(\"APPLICATION\")\n",
    "    pprint.pprint(application_items)\n",
    "    print()\n",
    "    print(f\"n_words_application = {str(n_words_application)}\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print(\"NOTE THAT THESE ARE THE NUMBERS OF DISTINCT WORDS\")\n",
    "    print(f\"n_words_description = {str(n_words_description)}\")\n",
    "    print(f\"n_words_application = {str(n_words_application)}\")\n",
    "    print()\n",
    "##endof:  if try_simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d7e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "for this_idx in range(max(len(description_items),\n",
    "                          len(application_items)\n",
    "                         )\n",
    "                      ):\n",
    "    this_rank = this_idx + 1\n",
    "    \n",
    "    if this_idx < len(description_items) - 1:\n",
    "        try:\n",
    "            this_description_word  = description_items[this_idx+1][0]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR desc word!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(description_items): {str(len(description_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(description_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        try:\n",
    "            this_description_count = description_items[this_idx+1][1]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR desc count!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(description_items): {str(len(description_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(description_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        this_description_rank  = this_rank\n",
    "    else:\n",
    "        this_description_word  = \"-- N/A --\"\n",
    "        this_description_count = \"-- N/A --\"\n",
    "        this_description_rank  = \"-- N/A --\"\n",
    "    ##endof:  if/else this_idx < len(description_items)\n",
    "    \n",
    "    if this_idx < len(application_items) - 1:\n",
    "        try:\n",
    "            this_application_word  = application_items[this_idx+1][0]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR appl word!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(application_items): {str(len(application_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(application_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        try:\n",
    "            this_application_count = application_items[this_idx+1][1]\n",
    "        except IndexError as ie:\n",
    "            print(\"OTHER ERROR appl count!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"len(application_items): {str(len(application_items))}\", file=sys.stdout)\n",
    "            end_of_data_bool_try = ( this_idx <= len(application_items) )\n",
    "            print(f\"end_of_data_bool_try: {str(end_of_data_bool_try)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "        \n",
    "        this_application_rank  = this_rank\n",
    "    else:\n",
    "        this_application_word  = \"-- N/A --\"\n",
    "        this_application_count = \"-- N/A --\"\n",
    "        this_application_rank  = \"-- N/A --\"\n",
    "    ##endof:  if/else this_idx < len(application_items)\n",
    "    \n",
    "    if this_description_word != \"-- N/A --\":\n",
    "        try:\n",
    "            table_version_desc.append([this_description_word, \n",
    "                                       this_description_count, \n",
    "                                       this_rank]\n",
    "                                     )\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR desc!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {str(this_idx)}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"this_description_word: {str(this_description_word)}\", file=sys.stdout)\n",
    "            print(f\"this_description_count: {str(this_description_count)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/except/finally\n",
    "        \n",
    "    ##endof:  if this_description_word != \"-- N/A --\"\n",
    "    \n",
    "    if this_application_word != \"-- N/A --\":\n",
    "        try:\n",
    "            table_version_appl.append([this_application_word, \n",
    "                                       this_application_count, \n",
    "                                       this_rank\n",
    "                                      ]\n",
    "                                     )\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR appl!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"this_idx: {this_idx}\", file=sys.stdout)\n",
    "            print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "            print(f\"this_application_word: {str(this_application_word)}\", file=sys.stdout)\n",
    "            print(f\"this_application_count: {str(this_application_count)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/except/finally\n",
    "        \n",
    "    ##endof:  if this_application_word != \"-- N/A --\"\n",
    "    \n",
    "    try:\n",
    "        table_version_both.append([this_rank, \n",
    "                                   this_description_word, \n",
    "                                   this_description_count,\n",
    "                                   this_application_word, \n",
    "                                   this_application_count\n",
    "                                  ]\n",
    "                                 )\n",
    "    except IndexError as ie:\n",
    "        print(\"ERROR both!\", file=sys.stdout)\n",
    "        print(str(ie), file=sys.stdout)\n",
    "        print(f\"this_idx: {this_idx}\", file=sys.stdout)\n",
    "        print(f\"this_rank: {str(this_rank)}\", file=sys.stdout)\n",
    "        print(f\"this_description_word: {str(this_description_word)}\", file=sys.stdout)\n",
    "        print(f\"this_description_count: {str(this_description_count)}\", file=sys.stdout)\n",
    "        print(f\"this_application_word: {str(this_application_word)}\", file=sys.stdout)\n",
    "        print(f\"this_application_count: {str(this_application_count)}\", file=sys.stdout)\n",
    "    finally:\n",
    "        pass\n",
    "    ##endof:  try/except/finally\n",
    "        \n",
    "##endof:  for this_idx in .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a9bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "##  Set up the display the first n_lines_to_display \n",
    "##+ of the tables, nicely\n",
    "\n",
    "#### This next one is the one you might change\n",
    "n_lines_to_display_orig = 25\n",
    "\n",
    "n_header_lines = 1\n",
    "n_lines_to_display = n_lines_to_display_orig\n",
    "n_lines_to_display_desc = n_lines_to_display_orig\n",
    "n_lines_to_display_appl = n_lines_to_display_orig\n",
    "\n",
    "do_cut_down_desc = ( n_lines_to_display_orig >\n",
    "                             len(table_version_desc) \n",
    "                   )\n",
    "if do_cut_down_desc:\n",
    "    n_lines_to_display_desc = len(table_version_desc)\n",
    "##endof:  if do_cut_down_desc\n",
    "\n",
    "do_cut_down_appl = ( n_lines_to_display_orig >\n",
    "                             len(table_version_appl) \n",
    "                   )\n",
    "if do_cut_down_appl:\n",
    "    n_lines_to_display_appl = len(table_version_appl)\n",
    "##endof:  if do_cut_down_appl\n",
    "\n",
    "# get headers\n",
    "display_table_desc = [table_version_desc[0]]\n",
    "display_table_appl = [table_version_appl[0]]\n",
    "display_table_both = [table_version_both[0]]\n",
    "\n",
    "if ( len(table_version_desc) - n_header_lines < n_lines_to_display or\n",
    "     len(table_version_appl) - n_header_lines < n_lines_to_display\n",
    "   ):\n",
    "    n_lines_to_display = min(len(table_version_desc) - n_header_lines,\n",
    "                             len(table_version_appl) - n_header_lines)\n",
    "##endof:  if <n_lines_conditions>\n",
    "\n",
    "\n",
    "\n",
    "for table_idx in range(n_header_lines, \n",
    "                       n_lines_to_display_orig + n_header_lines):\n",
    "    if table_idx - n_header_lines < n_lines_to_display_desc:\n",
    "        try:\n",
    "            display_table_desc.append(table_version_desc[table_idx])\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR display_table_desc!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_desc: {n_lines_to_display_desc}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_desc): {len(table_version_desc)}\", file=sys.stdout)\n",
    "        except Error as e:\n",
    "            print(\"DIFFERENT ERROR display_table_desc!\", file=sys.stdout)\n",
    "            print(str(e), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_desc: {n_lines_to_display_desc}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_desc): {len(table_version_desc)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "    ##endof:  if table_idx - n_header_lines > n_lines_to_display_desc\n",
    "    \n",
    "    if table_idx - n_header_lines < n_lines_to_display_appl:\n",
    "        try:\n",
    "            display_table_appl.append(table_version_appl[table_idx])\n",
    "        except IndexError as ie:\n",
    "            print(\"ERROR display_table_appl!\", file=sys.stdout)\n",
    "            print(str(ie), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_appl: {n_lines_to_display_appl}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_appl): {len(table_version_appl)}\", file=sys.stdout)\n",
    "        except Error as e:\n",
    "            print(\"DIFFERENT ERROR display_table_appl!\", file=sys.stdout)\n",
    "            print(str(e), file=sys.stdout)\n",
    "            print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "            print(f\"n_lines_to_display_appl: {n_lines_to_display_appl}\", file=sys.stdout)\n",
    "            print(f\"len(table_version_appl): {len(table_version_appl)}\", file=sys.stdout)\n",
    "        finally:\n",
    "            pass\n",
    "        ##endof:  try/catch/finally\n",
    "    ##endof:  if table_idx - n_header_lines > n_lines_to_display_appl\n",
    "    \n",
    "    try:\n",
    "        display_table_both.append(table_version_both[table_idx])\n",
    "    except IndexError as ie:\n",
    "        print(\"ERROR display_table_both!\", file=sys.stdout)\n",
    "        print(str(ie), file=sys.stdout)\n",
    "        print(f\"table_idx: {table_idx}\", file=sys.stdout)\n",
    "        print(f\"len(table_version_both): {len(table_version_both)}\", file=sys.stdout)\n",
    "    finally:\n",
    "        pass\n",
    "    ##endof:  try/catch/finally\n",
    "##endof:  for idx in range(<n_lines stuff>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len(table_version_desc): {len(table_version_desc)}\")\n",
    "print(f\"len(display_table_desc): {len(display_table_desc)}\")\n",
    "print()\n",
    "print(f\"len(table_version_appl): {len(table_version_appl)}\")\n",
    "print(f\"len(display_table_appl): {len(display_table_appl)}\")\n",
    "print()\n",
    "print(f\"len(table_version_both): {len(table_version_both)}\")\n",
    "print(f\"len(display_table_both): {len(display_table_both)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b5478",
   "metadata": {},
   "source": [
    "It's annoying me not to have a nice, aligned output for these 2d lists - basically, they're tables. I need to bring in some previous code that takes care of getting stuff printed nice. That will be after the Q&R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "long_dashes = \"-----------------------------------------------\"\n",
    "short_dashes = \"-----\"\n",
    "\n",
    "print()\n",
    "print(long_dashes)\n",
    "print(\"JOB DESCRIPTION (TOP 25)\")\n",
    "print(short_dashes)\n",
    "pprint.pprint(display_table_desc)\n",
    "print()\n",
    "print(long_dashes)\n",
    "print()\n",
    "print()\n",
    "print(long_dashes)\n",
    "print(\"JOB APPLICATION STUFF - RéSUMé AND COVER LETTER (TOP 25)\")\n",
    "print(short_dashes)\n",
    "pprint.pprint(display_table_appl)\n",
    "print()\n",
    "print(long_dashes)\n",
    "print()\n",
    "print()\n",
    "print(long_dashes)\n",
    "print(\"COMPARISON OF DESCRIPTION AND APPLICATION (TOP 25)\")\n",
    "print(short_dashes)\n",
    "pprint.pprint(display_table_both)\n",
    "print()\n",
    "print(long_dashes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1dc857",
   "metadata": {},
   "source": [
    "I should have stopped and `git`ted everything when I had the Ordered Dict (Minimum Viable Product), which was around 11:15-11:30, -0600. (It's now almost 13:00 -0600, which you'll see in the next timestamp.) Converting dictionaries to tables to whatever has given me a headache ; ). I'll `git` it here, where I can be to the point that works (except the errors in, not the previous cell, but the one before that). I think I could run the histogram with what I have (just after the `long_full_version` stuff), now. I'll do that after the `git` and a break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ff21fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d61a7e",
   "metadata": {},
   "source": [
    "Now, before the git and break, our output is\n",
    "\n",
    "`1690289898_20230725T125818-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e3ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# beg_and_end_timestamps = [[1690108602, 1690113083], \\\n",
    "#                           [1690197792, 1690198634], \\\n",
    "#                           [1690202804, 1690206455],\n",
    "#                           [1690220846, 1690223774],\n",
    "#                           [1690279068, 1690289898],\n",
    "#                          ]\n",
    "# \n",
    "# time_total(beg_and_end_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bc35cc",
   "metadata": {},
   "source": [
    "So, if I'd stopped and `git`ted at 11:30 - where I had the word-count dictionaries - instead of 13:00, I'd be at 4 hours 45 minutes. Still over, but definitely better. I'd have had to seen where the histograms put me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d482669e",
   "metadata": {},
   "source": [
    "## Time for histograms (or whatever the discretized version is)\n",
    "\n",
    "I'm back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ef5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122b935c",
   "metadata": {},
   "source": [
    "After the \"I'm back.\", the output from the previous command was\n",
    "\n",
    "`1690310816_20230725T184656-0600`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5092550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Next line only for Jupyter notebook.\n",
    "%matplotlib inline\n",
    "\n",
    "def get_histo_from_freq_dict(word_count_ordered_dict,\n",
    "                             n_top_words = 25,\n",
    "                             do_show_word_and_count_lists=False,\n",
    "                             axx=None\n",
    "                            ):\n",
    "    '''\n",
    "    @return  an axis from matplotlab (with the object - histogram - in it)\n",
    "    '''\n",
    "    \n",
    "    if axx is None:\n",
    "        fig = plt.figure(figsize=(10, 3))\n",
    "        axx = fig.add_subplot(111)\n",
    "    \n",
    "    counts_pre = list(word_count_ordered_dict.values())\n",
    "    words_pre  = list(word_count_ordered_dict.keys())\n",
    "    \n",
    "    counts = counts_pre[:n_top_words]\n",
    "    words  = words_pre[:n_top_words]\n",
    "    \n",
    "    ## making sure things were working\n",
    "    if do_show_word_and_count_lists:\n",
    "        print(f\"counts: {counts}\")\n",
    "        print(f\"words:  {words}\")\n",
    "    ##endof:  if do_show_word_and_count_lists\n",
    "    \n",
    "    x_words_coords = np.arange(len(words))\n",
    "    axx.bar(x_words_coords, counts, align='center')\n",
    "    \n",
    "    axx.set_xticks(x_words_coords)\n",
    "    axx.set_xticklabels(words, rotation=45, ha='right')\n",
    "    \n",
    "##endof:  get_histo_from_freq_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histo_from_freq_dict(description_word_counts, do_show_word_and_count_lists=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c005d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_histo_from_freq_dict(application_word_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d31453",
   "metadata": {},
   "source": [
    "The output histograms, in an image.\n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"first_QandR_word_frequency_plots.jpg\"\n",
    "       alt=\"The first pair of histograms - one for the job description, one for the job application - with word frequencies\"\n",
    "       width=\"100%\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecf6cde",
   "metadata": {},
   "source": [
    "Here is some idea of how they match. I hope it makes some sense. Darker green means an exact match; thinner dark green means a match with words that don't add much meaning; lighter green means it's a close match. \n",
    "\n",
    "<br/>\n",
    "<div>\n",
    "  <img src=\"first_QandR_word_frequency_plots_w_link_lines.jpg\"\n",
    "       alt=\"Word matches for the first pair of histograms.\"\n",
    "       width=\"100%\">\n",
    "</div>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7815f363",
   "metadata": {},
   "source": [
    "## There is the Q&R version, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325f399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# !powershell -c (Get-Date -UFormat \"%s_%Y%m%dT%H%M%S%Z00\") -replace '[.][0-9]*_', '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0760ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #######################\n",
    "# # No need to run again\n",
    "# #####\n",
    "# beg_and_end_timestamps = [[1690108602, 1690113083],\n",
    "#                           [1690197792, 1690198634],\n",
    "#                           [1690202804, 1690206455],\n",
    "#                           [1690220846, 1690223774],\n",
    "#                           [1690279068, 1690289898],\n",
    "#                           [1690310816, 1690313557],\n",
    "#                          ]\n",
    "# \n",
    "# time_total(beg_and_end_timestamps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334264ea",
   "metadata": {},
   "source": [
    "After the Q&R stuff is finished, the output was\n",
    "\n",
    "```\n",
    "total_seconds: 25473\n",
    "total_minutes: 424.55\n",
    "424 minutes and 33 seconds\n",
    "total_hours: 7.075833333333334\n",
    "4.55 minutes more than 7 hours\n",
    "7 hours and 4 minutes and 33 seconds\n",
    "```\n",
    "\n",
    "Really, with my work on doing stuff more quickly and less perfectly, I don't think that's too bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e01967",
   "metadata": {},
   "source": [
    "## Future Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779b13da",
   "metadata": {},
   "source": [
    "Look at ranking, counts, percentage, etc. for FamilySearch's (job description's) top 25 words as found in my (job application's) word counts, then vice-versa. Get rid of words that are necessary for grammar, but which don't matter too much in determining whether the two documents match up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73f0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
